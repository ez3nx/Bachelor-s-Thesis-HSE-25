{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-04T17:28:48.337061Z",
     "iopub.status.busy": "2025-05-04T17:28:48.336828Z",
     "iopub.status.idle": "2025-05-04T17:28:50.763307Z",
     "shell.execute_reply": "2025-05-04T17:28:50.762527Z",
     "shell.execute_reply.started": "2025-05-04T17:28:48.337041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{$SBER} ..................⬆️300,0 вопрос тольк...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{$SBER} на чем летим?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{$SBER} как прекрасен шортокрыл, посмотри....\\...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ 10 января Лукойл {$LKOH} рассмотрит итоги 20...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{$SBER}\\n \\nЧто быстрее, скорость света, или с...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  {$SBER} ..................⬆️300,0 вопрос тольк...    1.0\n",
       "1                              {$SBER} на чем летим?    0.0\n",
       "2  {$SBER} как прекрасен шортокрыл, посмотри....\\...   -1.0\n",
       "3  ✅ 10 января Лукойл {$LKOH} рассмотрит итоги 20...    0.0\n",
       "4  {$SBER}\\n \\nЧто быстрее, скорость света, или с...    0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# df = pd.read_csv(\"/kaggle/input/lora-df/data_for_lora.csv\")\n",
    "# df = pd.read_csv(\"/kaggle/input/data-for-lora-2/data_for_lora_2.csv\")\n",
    "# df = pd.read_csv(\"/kaggle/input/data-for-lora-3/data_for_lora_3.csv\")\n",
    "df = pd.read_csv(\"/kaggle/input/lora4-ygpt-only/data_for_lora_ygpt_only.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:28:53.910579Z",
     "iopub.status.busy": "2025-05-04T17:28:53.910104Z",
     "iopub.status.idle": "2025-05-04T17:28:54.510601Z",
     "shell.execute_reply": "2025-05-04T17:28:54.509732Z",
     "shell.execute_reply.started": "2025-05-04T17:28:53.910554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:28:59.241084Z",
     "iopub.status.busy": "2025-05-04T17:28:59.240221Z",
     "iopub.status.idle": "2025-05-04T17:29:00.941956Z",
     "shell.execute_reply": "2025-05-04T17:29:00.941114Z",
     "shell.execute_reply.started": "2025-05-04T17:28:59.241058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Структура DatasetDict:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', '__index_level_0__'],\n",
      "        num_rows: 28342\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', '__index_level_0__'],\n",
      "        num_rows: 3150\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', '__index_level_0__'],\n",
      "        num_rows: 3500\n",
      "    })\n",
      "})\n",
      "\n",
      "Пример записи из обучающего набора:\n",
      "{'text': '{$SBER} оно где-то рядом, смотрите во всех акциях РФ)', 'labels': 0.0, '__index_level_0__': 23574}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, Value\n",
    "\n",
    "train_df = pd.DataFrame({'text': X_train, 'labels': y_train})\n",
    "val_df = pd.DataFrame({'text': X_val, 'labels': y_val})\n",
    "test_df = pd.DataFrame({'text': X_test, 'labels': y_test})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# --- Проверка ---\n",
    "print(\"\\nСтруктура DatasetDict:\")\n",
    "print(dataset_dict)\n",
    "print(\"\\nПример записи из обучающего набора:\")\n",
    "print(dataset_dict['train'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:29:05.851133Z",
     "iopub.status.busy": "2025-05-04T17:29:05.850688Z",
     "iopub.status.idle": "2025-05-04T17:29:07.451003Z",
     "shell.execute_reply": "2025-05-04T17:29:07.450304Z",
     "shell.execute_reply.started": "2025-05-04T17:29:05.851108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def map_labels(example):\n",
    "    example['labels'] = label_map[example['labels']]\n",
    "    return example\n",
    "\n",
    "label_map = {-1: 0, 0: 1, 1: 2}\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "num_labels = len(label_map)\n",
    "\n",
    "dataset_dict = dataset_dict.map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:29:31.765891Z",
     "iopub.status.busy": "2025-05-04T17:29:31.765199Z",
     "iopub.status.idle": "2025-05-04T17:29:50.595324Z",
     "shell.execute_reply": "2025-05-04T17:29:50.594670Z",
     "shell.execute_reply.started": "2025-05-04T17:29:31.765866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизатор загружен!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "# Загружаем токенизатор от исходной модели\n",
    "try:\n",
    "    model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    clear_output()\n",
    "    print(\"Токенизатор загружен!\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:29:50.597100Z",
     "iopub.status.busy": "2025-05-04T17:29:50.596590Z",
     "iopub.status.idle": "2025-05-04T17:29:54.558960Z",
     "shell.execute_reply": "2025-05-04T17:29:54.558246Z",
     "shell.execute_reply.started": "2025-05-04T17:29:50.597080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Функция токенизации\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False) # Паддинг будет позже с DataCollator\n",
    "\n",
    "# Применяем токенизацию ко всем данным\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "# Удаляем ненужную колонку 'text', так как она уже преобразована в input_ids/attention_mask\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "\n",
    "print(\"Типы данных ДО кастинга:\", dataset_dict['train'].features) # Посмотреть исходный тип\n",
    "dataset_dict = dataset_dict.cast_column(\"labels\", Value('int64'))\n",
    "print(\"Типы данных ПОСЛЕ кастинга:\", dataset_dict['train'].features)\n",
    "\n",
    "# Устанавливаем формат для PyTorch\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Data Collator для динамического паддинга батчей\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:29:54.560024Z",
     "iopub.status.busy": "2025-05-04T17:29:54.559789Z",
     "iopub.status.idle": "2025-05-04T17:29:54.567369Z",
     "shell.execute_reply": "2025-05-04T17:29:54.566637Z",
     "shell.execute_reply.started": "2025-05-04T17:29:54.560001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class CustomDataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Кастомный Data Collator, который использует паддинг токенизатора\n",
    "    и гарантирует, что 'labels' будут иметь тип torch.long.\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\" # Возвращаем PyTorch тензоры\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # Извлекаем метки до обработки остальных признаков\n",
    "        labels = None\n",
    "        if \"labels\" in features[0].keys():\n",
    "            labels = [feature[\"labels\"] for feature in features] # Собираем метки\n",
    "\n",
    "        # Используем токенизатор для паддинга input_ids, attention_mask и т.д.\n",
    "        features_for_padding = [{k: v for k, v in feature.items() if k != 'labels'} for feature in features]\n",
    "        batch = self.tokenizer.pad(\n",
    "            features_for_padding,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "\n",
    "        # Добавляем метки обратно в батч, ПРЕОБРАЗУЯ ИХ В TENSOR ТИПА LONG\n",
    "        if labels is not None:\n",
    "            # batch[\"labels\"] = torch.tensor(labels, dtype=torch.long) # <<< Гарантируем torch.long\n",
    "            batch[\"labels\"] = torch.stack(labels)\n",
    "            batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
    "\n",
    "        return batch\n",
    "\n",
    "custom_data_collator = CustomDataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- ЗАГРУЗКА БАЗОВОЙ МОДЕЛИ ---\n",
    "# Важно: указываем новое количество классов (num_labels=3)\n",
    "# и ignore_mismatched_sizes=True, чтобы игнорировать несовпадение размера\n",
    "# выходного слоя классификатора (у предобученной модели 5 выходов, нам нужно 3).\n",
    "# Это приведет к инициализации *нового* случайного слоя классификации поверх\n",
    "# предобученных слоев DistilBERT.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    # Добавим маппинги id <-> label для удобства\n",
    "    id2label={i: f\"LABEL_{reverse_label_map[i]}\" for i in range(num_labels)},\n",
    "    label2id={f\"LABEL_{reverse_label_map[i]}\": i for i in range(num_labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# --- КОНФИГУРАЦИЯ LoRA ---\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, # Тип задачи - классификация последовательностей\n",
    "    r=16,                       # Ранг матриц адаптера (типичные значения: 8, 16, 32)\n",
    "    lora_alpha=32,              # Коэффициент масштабирования (часто 2*r)\n",
    "    lora_dropout=0.1,           # Dropout для LoRA слоев\n",
    "    bias=\"none\",                # Обычно не обучаем смещения в LoRA ('none' или 'all')\n",
    "    # Указываем модули, к которым применяем LoRA.\n",
    "    # Для DistilBERT это обычно 'q_lin' и 'v_lin' в слоях внимания.\n",
    "    # Можно проверить названия командой: print(model)\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    ")\n",
    "\n",
    "# --- ПРИМЕНЕНИЕ LoRA К МОДЕЛИ ---\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"\\nПараметры модели после применения LoRA:\")\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:29:54.569609Z",
     "iopub.status.busy": "2025-05-04T17:29:54.569237Z",
     "iopub.status.idle": "2025-05-04T17:29:59.294434Z",
     "shell.execute_reply": "2025-05-04T17:29:59.293712Z",
     "shell.execute_reply.started": "2025-05-04T17:29:54.569582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:29:59.295756Z",
     "iopub.status.busy": "2025-05-04T17:29:59.295496Z",
     "iopub.status.idle": "2025-05-04T17:30:05.132333Z",
     "shell.execute_reply": "2025-05-04T17:30:05.131795Z",
     "shell.execute_reply.started": "2025-05-04T17:29:59.295733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# # --- МЕТРИКИ ---\n",
    "# # accuracy_metric = evaluate.load(\"accuracy\")\n",
    "# # f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# # def compute_metrics(eval_pred):\n",
    "# #     predictions, labels = eval_pred\n",
    "# #     # Получаем предсказанный класс (индекс с максимальной логитом)\n",
    "# #     predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "# #     acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "# #     f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\") # Используем weighted F1 для многоклассовой задачи\n",
    "    \n",
    "# #     return {\n",
    "#         \"accuracy\": acc[\"accuracy\"],\n",
    "# #         \"f1_weighted\": f1[\"f1\"],\n",
    "# #     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:30:05.133768Z",
     "iopub.status.busy": "2025-05-04T17:30:05.133072Z",
     "iopub.status.idle": "2025-05-04T17:30:13.477645Z",
     "shell.execute_reply": "2025-05-04T17:30:13.477037Z",
     "shell.execute_reply.started": "2025-05-04T17:30:05.133746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mez3nx\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient # Импортируем клиент для доступа к секретам\n",
    "\n",
    "# Получаем доступ к секретам пользователя\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "# Получаем значение секрета по его Label (имени), которое вы задали\n",
    "# Убедитесь, что 'WANDB_API_KEY' точно совпадает с Label, который вы ввели на шаге 2\n",
    "wandb_api_key = user_secrets.get_secret(\"wb_lora\") \n",
    "\n",
    "# Логинимся, передавая ключ напрямую в функцию\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:30:13.478872Z",
     "iopub.status.busy": "2025-05-04T17:30:13.478308Z",
     "iopub.status.idle": "2025-05-04T17:30:13.482874Z",
     "shell.execute_reply": "2025-05-04T17:30:13.482129Z",
     "shell.execute_reply.started": "2025-05-04T17:30:13.478852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"sentiment_lora_finetuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- АРГУМЕНТЫ ОБУЧЕНИЯ ---\n",
    "# Настройте пути, гиперпараметры под свои нужды и ресурсы\n",
    "output_dir = \"./sentiment_lora_finetuned\"\n",
    "learning_rate = 2e-4 # LoRA часто требует бОльший learning rate, чем full fine-tuning\n",
    "batch_size = 64\n",
    "num_train_epochs = 5 # Обычно достаточно нескольких эпох для LoRA\n",
    "weight_decay = 0.01\n",
    "\n",
    "model_name_short = model_name.split('/')[-1] # \"multilingual-sentiment-analysis\"\n",
    "run_name = f\"{model_name_short}-lora-r{lora_config.r}-alpha{lora_config.lora_alpha}-lr{learning_rate}-epochs{num_train_epochs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    label_names=[\"labels\"],\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\", # Оценивать после каждой эпохи\n",
    "    save_strategy=\"epoch\",       # Сохранять после каждой эпохи\n",
    "    logging_strategy=\"epoch\",    # Логировать после каждой эпохи\n",
    "    load_best_model_at_end=True, # Загрузить лучшую модель в конце обучения\n",
    "    metric_for_best_model=\"f1_weighted\", # Метрика для выбора лучшей модели\n",
    "    push_to_hub=False,           # Установите True, если хотите загрузить на Hugging Face Hub\n",
    "    fp16=torch.cuda.is_available(), # Использовать смешанную точность, если доступен GPU\n",
    "    # --- КЛЮЧЕВЫЕ ИЗМЕНЕНИЯ ДЛЯ W&B ---\n",
    "    report_to=\"wandb\",              # <--- Указываем W&B как платформу для логирования\n",
    "    run_name=run_name,              # <--- Задаем имя запуска (отобразится в W&B UI)\n",
    "    # ---------------------------------\n",
    "    # Остальные аргументы по необходимости...\n",
    "    logging_dir='./logs', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model,               # Используем PEFT модель\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=custom_data_collator,    # Для динамического паддинга\n",
    "    compute_metrics=compute_metrics,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade wandb transformers accelerate datasets peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- ЗАПУСК ОБУЧЕНИЯ ---\n",
    "print(\"\\nНачало обучения...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- СОХРАНЕНИЕ АДАПТЕРА ---\n",
    "# Trainer автоматически сохранит лучший адаптер в output_dir/best_model\n",
    "# Можно также сохранить явно последнюю версию адаптера:\n",
    "adapter_path = f\"{output_dir}/final_adapter\"\n",
    "peft_model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path) # Сохраним и токенизатор рядом\n",
    "print(f\"Обучение завершено. Финальный адаптер LoRA сохранен в: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    # Добавим маппинги id <-> label для удобства\n",
    "    id2label={i: f\"LABEL_{reverse_label_map[i]}\" for i in range(num_labels)},\n",
    "    label2id={f\"LABEL_{reverse_label_map[i]}\": i for i in range(num_labels)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config_2 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,                       \n",
    "    lora_alpha=32,              \n",
    "    lora_dropout=0.1,           \n",
    "    bias=\"none\",         \n",
    "    target_modules=[\"q_lin\", \"v_lin\", \"out_lin\", \"lin1\", \"lin2\"],\n",
    ")\n",
    "\n",
    "# --- ПРИМЕНЕНИЕ LoRA К МОДЕЛИ ---\n",
    "peft_model_2 = get_peft_model(model_2, lora_config_2)\n",
    "\n",
    "print(\"\\nПараметры модели после применения LoRA:\")\n",
    "peft_model_2.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:30:18.579849Z",
     "iopub.status.busy": "2025-05-04T17:30:18.579556Z",
     "iopub.status.idle": "2025-05-04T17:30:20.592975Z",
     "shell.execute_reply": "2025-05-04T17:30:20.592467Z",
     "shell.execute_reply.started": "2025-05-04T17:30:18.579828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Загружаем необходимые метрики из evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\") # Добавили Precision\n",
    "recall_metric = evaluate.load(\"recall\")     # Добавили Recall\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Вычисляет accuracy, weighted precision, recall и F1 для многоклассовой задачи.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions_logits, labels = eval_pred\n",
    "    # Получаем предсказанный класс (индекс с максимальной логитом)\n",
    "    predictions = np.argmax(predictions_logits, axis=1)\n",
    "\n",
    "    # Рассчитываем метрики\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Используем average=\"weighted\" для Precision, Recall, F1\n",
    "    # чтобы учесть количество примеров в каждом классе\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "    # Возвращаем словарь с метриками\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "        \"precision_weighted\": precision[\"precision\"], # Добавили ключ для precision\n",
    "        \"recall_weighted\": recall[\"recall\"],       # Добавили ключ для recall\n",
    "        \"f1_weighted\": f1[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"./sentiment_lora_finetuned_2\"\n",
    "learning_rate = 1e-4 # LoRA часто требует бОльший learning rate, чем full fine-tuning\n",
    "batch_size = 32\n",
    "num_train_epochs = 5 # Обычно достаточно нескольких эпох для LoRA\n",
    "weight_decay = 0.001\n",
    "\n",
    "model_name_short = model_name.split('/')[-1] # \"multilingual-sentiment-analysis\"\n",
    "run_name = f\"{model_name_short}-lora-r{lora_config.r}-alpha{lora_config.lora_alpha}-lr{learning_rate}-epochs{num_train_epochs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,          # Начальная скорость обучения\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    label_names=[\"labels\"],\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    push_to_hub=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "\n",
    "    # --- ДОБАВЛЕННЫЕ ПАРАМЕТРЫ ШЕДУЛЕРА ---\n",
    "    lr_scheduler_type='cosine',      # Тип планировщика: косинусный\n",
    "    warmup_ratio=0.1,                # Доля шагов для прогрева (10% от общих шагов обучения)\n",
    "    # Или можно использовать warmup_steps=N, если вы знаете точное число шагов прогрева (N)\n",
    "    # ---------------------------------------\n",
    "\n",
    "    # --- Параметры W&B ---\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    "    logging_dir='./logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model_2,               # Используем PEFT модель\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=custom_data_collator,    # Для динамического паддинга\n",
    "    compute_metrics=compute_metrics,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- ЗАПУСК ОБУЧЕНИЯ ---\n",
    "print(\"\\nНачало обучения...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- СОХРАНЕНИЕ АДАПТЕРА ---\n",
    "# Trainer автоматически сохранит лучший адаптер в output_dir/best_model\n",
    "# Можно также сохранить явно последнюю версию адаптера:\n",
    "adapter_path = f\"{output_dir}/final_adapter\"\n",
    "peft_model_2.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path) # Сохраним и токенизатор рядом\n",
    "print(f\"Обучение завершено. Финальный адаптер LoRA сохранен в: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:30:34.668074Z",
     "iopub.status.busy": "2025-05-04T17:30:34.667517Z",
     "iopub.status.idle": "2025-05-04T17:30:37.378366Z",
     "shell.execute_reply": "2025-05-04T17:30:37.377774Z",
     "shell.execute_reply.started": "2025-05-04T17:30:34.668050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_3 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    # Добавим маппинги id <-> label для удобства\n",
    "    id2label={i: f\"LABEL_{reverse_label_map[i]}\" for i in range(num_labels)},\n",
    "    label2id={f\"LABEL_{reverse_label_map[i]}\": i for i in range(num_labels)}\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "lora_config_3 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=64,                       \n",
    "    lora_alpha=128,              \n",
    "    lora_dropout=0.1,           \n",
    "    bias=\"none\",         \n",
    "    target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\", \"lin1\", \"lin2\"],\n",
    ")\n",
    "\n",
    "# --- ПРИМЕНЕНИЕ LoRA К МОДЕЛИ ---\n",
    "peft_model_3 = get_peft_model(model_3, lora_config_3)\n",
    "\n",
    "print(\"\\nПараметры модели после применения LoRA:\")\n",
    "peft_model_3.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:34:12.137746Z",
     "iopub.status.busy": "2025-05-04T17:34:12.137072Z",
     "iopub.status.idle": "2025-05-04T17:34:12.175013Z",
     "shell.execute_reply": "2025-05-04T17:34:12.174456Z",
     "shell.execute_reply.started": "2025-05-04T17:34:12.137718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"./sentiment_lora_finetuned_BIG_SET\"\n",
    "learning_rate = 2e-4 # LoRA часто требует бОльший learning rate, чем full fine-tuning\n",
    "batch_size = 32\n",
    "num_train_epochs = 7 # Обычно достаточно нескольких эпох для LoRA\n",
    "weight_decay = 0.001\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "model_name_short = model_name.split('/')[-1] # \"multilingual-sentiment-analysis\"\n",
    "run_name = f\"{model_name_short}-lora-r{lora_config_3.r}-alpha{lora_config_3.lora_alpha}-lr{learning_rate}-epochs{num_train_epochs}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,          # Начальная скорость обучения\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    label_names=[\"labels\"],\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    push_to_hub=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.2,\n",
    "    # --- ДОБАВЛЕННЫЙ ПАРАМЕТР ---\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps, # Указываем шаги аккумуляции\n",
    "    # --------------------------\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    "    logging_dir='./logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:34:16.505975Z",
     "iopub.status.busy": "2025-05-04T17:34:16.505197Z",
     "iopub.status.idle": "2025-05-04T17:34:16.859601Z",
     "shell.execute_reply": "2025-05-04T17:34:16.858902Z",
     "shell.execute_reply.started": "2025-05-04T17:34:16.505952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model_3,               # Используем PEFT модель\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=custom_data_collator,    # Для динамического паддинга data_collator\n",
    "    compute_metrics=compute_metrics,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:34:21.036841Z",
     "iopub.status.busy": "2025-05-04T17:34:21.036089Z",
     "iopub.status.idle": "2025-05-04T18:54:52.188306Z",
     "shell.execute_reply": "2025-05-04T18:54:52.187476Z",
     "shell.execute_reply.started": "2025-05-04T17:34:21.036815Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Начало обучения...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250504_173421-51qf0tjy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/51qf0tjy' target=\"_blank\">multilingual-sentiment-analysis-lora-r64-alpha128-lr0.0002-epochs7</a></strong> to <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/51qf0tjy' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/51qf0tjy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [770/770 1:20:15, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>0.823498</td>\n",
       "      <td>0.637460</td>\n",
       "      <td>0.640601</td>\n",
       "      <td>0.637460</td>\n",
       "      <td>0.636775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.777600</td>\n",
       "      <td>0.722078</td>\n",
       "      <td>0.689206</td>\n",
       "      <td>0.690140</td>\n",
       "      <td>0.689206</td>\n",
       "      <td>0.689331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.673300</td>\n",
       "      <td>0.700395</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>0.705085</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>0.702327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0.690893</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>0.710926</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>0.707477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.526600</td>\n",
       "      <td>0.699082</td>\n",
       "      <td>0.717460</td>\n",
       "      <td>0.718003</td>\n",
       "      <td>0.717460</td>\n",
       "      <td>0.717522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.471500</td>\n",
       "      <td>0.718486</td>\n",
       "      <td>0.716190</td>\n",
       "      <td>0.717527</td>\n",
       "      <td>0.716190</td>\n",
       "      <td>0.716153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.442600</td>\n",
       "      <td>0.728787</td>\n",
       "      <td>0.713016</td>\n",
       "      <td>0.713090</td>\n",
       "      <td>0.713016</td>\n",
       "      <td>0.713032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▇▇███</td></tr><tr><td>eval/f1_weighted</td><td>▁▆▇▇███</td></tr><tr><td>eval/loss</td><td>█▃▂▁▁▂▃</td></tr><tr><td>eval/precision_weighted</td><td>▁▅▇▇███</td></tr><tr><td>eval/recall_weighted</td><td>▁▆▇▇███</td></tr><tr><td>eval/runtime</td><td>█▇▁▁▂▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▂██▇█▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▂██▇█▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▂▂▂█▅▄</td></tr><tr><td>train/learning_rate</td><td>▆█▇▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.71302</td></tr><tr><td>eval/f1_weighted</td><td>0.71303</td></tr><tr><td>eval/loss</td><td>0.72879</td></tr><tr><td>eval/precision_weighted</td><td>0.71309</td></tr><tr><td>eval/recall_weighted</td><td>0.71302</td></tr><tr><td>eval/runtime</td><td>33.4938</td></tr><tr><td>eval/samples_per_second</td><td>94.047</td></tr><tr><td>eval/steps_per_second</td><td>1.493</td></tr><tr><td>total_flos</td><td>2.753819657718221e+16</td></tr><tr><td>train/epoch</td><td>6.99323</td></tr><tr><td>train/global_step</td><td>770</td></tr><tr><td>train/grad_norm</td><td>101724.78125</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4426</td></tr><tr><td>train_loss</td><td>0.63408</td></tr><tr><td>train_runtime</td><td>4829.0947</td></tr><tr><td>train_samples_per_second</td><td>41.083</td></tr><tr><td>train_steps_per_second</td><td>0.159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">multilingual-sentiment-analysis-lora-r64-alpha128-lr0.0002-epochs7</strong> at: <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/51qf0tjy' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/51qf0tjy</a><br> View project at: <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_173421-51qf0tjy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение завершено. Финальный адаптер LoRA сохранен в: ./sentiment_lora_finetuned_BIG_SET/final_adapter\n"
     ]
    }
   ],
   "source": [
    "# --- ЗАПУСК ОБУЧЕНИЯ ---\n",
    "print(\"\\nНачало обучения...\")\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "# --- СОХРАНЕНИЕ АДАПТЕРА ---\n",
    "# Trainer автоматически сохранит лучший адаптер в output_dir/best_model\n",
    "# Можно также сохранить явно последнюю версию адаптера:\n",
    "adapter_path = f\"{output_dir}/final_adapter\"\n",
    "peft_model_3.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path) # Сохраним и токенизатор рядом\n",
    "print(f\"Обучение завершено. Финальный адаптер LoRA сохранен в: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:04:39.567590Z",
     "iopub.status.busy": "2025-05-04T19:04:39.566904Z",
     "iopub.status.idle": "2025-05-04T19:05:50.658310Z",
     "shell.execute_reply": "2025-05-04T19:05:50.657727Z",
     "shell.execute_reply.started": "2025-05-04T19:04:39.567566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Маппинг 5->3 классов: {0: 0, 1: 0, 2: 1, 3: 2, 4: 2}\n",
      "\n",
      "Загрузка ОРИГИНАЛЬНОЙ 5-классовой модели tabularisai/multilingual-sentiment-analysis...\n",
      "Оригинальная модель загружена.\n",
      "\n",
      "Запуск оценки ОРИГИНАЛЬНОЙ модели на валидационном наборе...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Метрики ОРИГИНАЛЬНОЙ модели (преобразованные к 3 классам) ---\n",
      "eval_loss: 1.645286\n",
      "eval_accuracy: 0.447302\n",
      "eval_precision_weighted: 0.447048\n",
      "eval_recall_weighted: 0.447302\n",
      "eval_f1_weighted: 0.443735\n",
      "eval_runtime: 29.016000\n",
      "eval_samples_per_second: 108.561000\n",
      "eval_steps_per_second: 1.723000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250504_190509-z1yriq2d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ez3nx/uncategorized/runs/z1yriq2d' target=\"_blank\">star-midichlorian-1</a></strong> to <a href='https://wandb.ai/ez3nx/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ez3nx/uncategorized' target=\"_blank\">https://wandb.ai/ez3nx/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ez3nx/uncategorized/runs/z1yriq2d' target=\"_blank\">https://wandb.ai/ez3nx/uncategorized/runs/z1yriq2d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Сравнение F1 Weighted ---\n",
      "Оригинальная модель: 0.443735\n",
      "LoRA модель (лучшая): 0.717522\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# --- Убедитесь, что эти объекты существуют из вашего кода обучения LoRA ---\n",
    "# tokenized_datasets[\"validation\"] - ваш валидационный датасет, токенизированный,\n",
    "#                                     с колонкой 'labels' (значения 0, 1, 2)\n",
    "# label_map = {-1: 0, 0: 1, 1: 2} # Ваше отображение\n",
    "# reverse_label_map = {0: -1, 1: 0, 2: 1} # Ваше обратное отображение\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- 1. Определяем функцию метрик для ОРИГИНАЛЬНОЙ модели ---\n",
    "#    Она будет мапить 5 предсказанных классов в 3\n",
    "\n",
    "# Определяем маппинг: индекс оригинального класса -> индекс нашего класса (0, 1, 2)\n",
    "original_5_to_3_map = {\n",
    "    0: label_map[-1], # Very Negative -> Наш Negative (0)\n",
    "    1: label_map[-1], # Negative      -> Наш Negative (0)\n",
    "    2: label_map[0],  # Neutral       -> Наш Neutral (1)\n",
    "    3: label_map[1],  # Positive      -> Наш Positive (2)\n",
    "    4: label_map[1]   # Very Positive -> Наш Positive (2)\n",
    "}\n",
    "print(f\"Маппинг 5->3 классов: {original_5_to_3_map}\")\n",
    "\n",
    "def compute_metrics_original_model(eval_pred):\n",
    "    \"\"\"\n",
    "    Вычисляет метрики для 3 классов, получая на вход логиты для 5 классов\n",
    "    и истинные метки для 3 классов.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logits_5_class, labels_3_class = eval_pred # labels_3_class уже 0, 1, 2\n",
    "\n",
    "        # Получаем предсказанные классы из 5 логитов (индексы 0-4)\n",
    "        predictions_5_class_indices = np.argmax(logits_5_class, axis=1)\n",
    "\n",
    "        # Маппим предсказания 5 классов в наши 3 класса (индексы 0-2)\n",
    "        predictions_3_class_mapped = np.array(\n",
    "            [original_5_to_3_map[p_5_idx] for p_5_idx in predictions_5_class_indices]\n",
    "        )\n",
    "\n",
    "        # Вычисляем метрики, сравнивая смапленные 3-класс. предсказания с 3-класс. метками\n",
    "        acc = accuracy_metric.compute(predictions=predictions_3_class_mapped, references=labels_3_class)\n",
    "        precision = precision_metric.compute(predictions=predictions_3_class_mapped, references=labels_3_class, average=\"weighted\")\n",
    "        recall = recall_metric.compute(predictions=predictions_3_class_mapped, references=labels_3_class, average=\"weighted\")\n",
    "        f1 = f1_metric.compute(predictions=predictions_3_class_mapped, references=labels_3_class, average=\"weighted\")\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": acc[\"accuracy\"],\n",
    "            \"precision_weighted\": precision[\"precision\"],\n",
    "            \"recall_weighted\": recall[\"recall\"],\n",
    "            \"f1_weighted\": f1[\"f1\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Ошибка в compute_metrics_original_model: {e} !!!\")\n",
    "        return {\"metric_computation_error\": 1}\n",
    "\n",
    "\n",
    "# --- 2. Загрузка ОРИГИНАЛЬНОЙ модели и токенизатора ---\n",
    "model_name_original = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "print(f\"\\nЗагрузка ОРИГИНАЛЬНОЙ 5-классовой модели {model_name_original}...\")\n",
    "# Загружаем как есть, с 5 классами по умолчанию\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(model_name_original)\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_name_original)\n",
    "print(\"Оригинальная модель загружена.\")\n",
    "\n",
    "\n",
    "# --- 3. Подготовка к оценке ---\n",
    "# !!! ВАЖНО: Убедитесь, что ваш `tokenized_datasets[\"validation\"]` был токенизирован\n",
    "#           с помощью ТОГО ЖЕ `original_tokenizer`. Если нет, его нужно перетокенизировать!\n",
    "# Пример перетокенизации, если нужно (замените dataset_dict['validation'] на ваш исходный валидационный датасет):\n",
    "# print(\"Перетокенизация валидационного набора оригинальным токенизатором...\")\n",
    "# validation_dataset_retokenized = dataset_dict['validation'].map(\n",
    "#     lambda ex: original_tokenizer(ex[\"text\"], truncation=True, padding=False, max_length=512), batched=True\n",
    "# )\n",
    "# validation_dataset_retokenized = validation_dataset_retokenized.remove_columns([\"text\"]) # и другие ненужные колонки\n",
    "# # Убедимся, что метки 'labels' типа int64\n",
    "# validation_dataset_retokenized = validation_dataset_retokenized.cast_column(\"labels\", Value('int64'))\n",
    "# validation_dataset_retokenized.set_format(\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# eval_dataset_for_original = validation_dataset_retokenized\n",
    "# print(\"Перетокенизация завершена.\")\n",
    "\n",
    "# Если вы уверены, что токенизация была одинаковой:\n",
    "eval_dataset_for_original = tokenized_datasets[\"validation\"]\n",
    "\n",
    "\n",
    "# Используем минимальные аргументы для оценки\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./eval_original_model_temp\", # Просто временная папка\n",
    "    per_device_eval_batch_size=32,        # Можно увеличить для скорости\n",
    "    report_to=\"none\",                     # Не логируем эту оценку\n",
    ")\n",
    "\n",
    "# Создаем Trainer для ОЦЕНКИ оригинальной модели\n",
    "eval_trainer_original = Trainer(\n",
    "    model=original_model,                   # <<< ОРИГИНАЛЬНАЯ модель\n",
    "    args=eval_args,\n",
    "    eval_dataset=eval_dataset_for_original, # <<< Ваш валидационный набор\n",
    "    processing_class=original_tokenizer,           # <<< ОРИГИНАЛЬНЫЙ токенизатор\n",
    "    data_collator=custom_data_collator,\n",
    "    compute_metrics=compute_metrics_original_model # <<< СПЕЦИАЛЬНАЯ функция метрик\n",
    ")\n",
    "\n",
    "# --- 4. Запуск оценки ---\n",
    "print(\"\\nЗапуск оценки ОРИГИНАЛЬНОЙ модели на валидационном наборе...\")\n",
    "original_model_metrics = eval_trainer_original.evaluate()\n",
    "\n",
    "print(\"\\n--- Метрики ОРИГИНАЛЬНОЙ модели (преобразованные к 3 классам) ---\")\n",
    "for key, value in original_model_metrics.items():\n",
    "    print(f\"{key}: {value:.6f}\")\n",
    "\n",
    "# --- 5. Сравнение (если у вас есть метрики последней LoRA модели) ---\n",
    "# Предположим, лучшие метрики вашей LoRA модели хранятся в словаре best_lora_metrics\n",
    "wandb.init()\n",
    "best_lora_metrics = trainer.evaluate() # Если Trainer еще доступен и содержит лучшую модель\n",
    "# или загрузите из логов W&B/файла\n",
    "\n",
    "print(\"\\n--- Сравнение F1 Weighted ---\")\n",
    "print(f\"Оригинальная модель: {original_model_metrics.get('eval_f1_weighted', 'N/A'):.6f}\")\n",
    "print(f\"LoRA модель (лучшая): {best_lora_metrics.get('eval_f1_weighted', 'N/A'):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:06:08.025772Z",
     "iopub.status.busy": "2025-05-04T19:06:08.025130Z",
     "iopub.status.idle": "2025-05-04T19:06:08.031581Z",
     "shell.execute_reply": "2025-05-04T19:06:08.030783Z",
     "shell.execute_reply.started": "2025-05-04T19:06:08.025748Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6990819573402405,\n",
       " 'eval_accuracy': 0.7174603174603175,\n",
       " 'eval_precision_weighted': 0.7180032833786785,\n",
       " 'eval_recall_weighted': 0.7174603174603175,\n",
       " 'eval_f1_weighted': 0.717522136966659,\n",
       " 'eval_runtime': 35.2164,\n",
       " 'eval_samples_per_second': 89.447,\n",
       " 'eval_steps_per_second': 1.42,\n",
       " 'epoch': 6.993227990970655}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lora_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:07:25.294692Z",
     "iopub.status.busy": "2025-05-04T19:07:25.294123Z",
     "iopub.status.idle": "2025-05-04T19:07:25.300136Z",
     "shell.execute_reply": "2025-05-04T19:07:25.299477Z",
     "shell.execute_reply.started": "2025-05-04T19:07:25.294669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6452864408493042,\n",
       " 'eval_accuracy': 0.4473015873015873,\n",
       " 'eval_precision_weighted': 0.4470480131892384,\n",
       " 'eval_recall_weighted': 0.4473015873015873,\n",
       " 'eval_f1_weighted': 0.4437350351888902,\n",
       " 'eval_runtime': 29.016,\n",
       " 'eval_samples_per_second': 108.561,\n",
       " 'eval_steps_per_second': 1.723}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:13:45.104010Z",
     "iopub.status.busy": "2025-05-04T19:13:45.103252Z",
     "iopub.status.idle": "2025-05-04T19:13:45.121314Z",
     "shell.execute_reply": "2025-05-04T19:13:45.120358Z",
     "shell.execute_reply.started": "2025-05-04T19:13:45.103986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfffe224c4634a3298d3472f0c648fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login() # Попросит ввести ваш токен доступа (из настроек HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:11:32.777789Z",
     "iopub.status.busy": "2025-05-04T19:11:32.777205Z",
     "iopub.status.idle": "2025-05-04T19:11:33.164918Z",
     "shell.execute_reply": "2025-05-04T19:11:33.164321Z",
     "shell.execute_reply.started": "2025-05-04T19:11:32.777763Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at tabularisai/multilingual-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "adapter_path = \"/kaggle/working/sentiment_lora_finetuned_BIG_SET/final_adapter\" # Укажите правильный путь к папке с лучшим адаптером\n",
    "num_labels = 3\n",
    "# ... (определите reverse_label_map_inference) ...\n",
    "\n",
    "# Загружаем базу\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    id2label={i: f\"LABEL_{reverse_label_map[i]}\" for i in range(num_labels)},\n",
    "    label2id={f\"LABEL_{reverse_label_map[i]}\": i for i in range(num_labels)}\n",
    ")\n",
    "\n",
    "# Применяем адаптер\n",
    "inference_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Загружаем токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "\n",
    "# Убедитесь, что у токенизатора установлен pad_token, если он нужен\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "     tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T19:13:50.599435Z",
     "iopub.status.busy": "2025-05-04T19:13:50.598789Z",
     "iopub.status.idle": "2025-05-04T19:13:56.093158Z",
     "shell.execute_reply": "2025-05-04T19:13:56.092483Z",
     "shell.execute_reply.started": "2025-05-04T19:13:50.599395Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Публикация адаптера LoRA и токенизатора в репозиторий: ez3nx/multilingual-sentiment-3class-lora-adapter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d1414e6dbd4da8a4af6ff652a488a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/23.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6e220e58ff4e10bf5eb62a02e9f29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Публикация завершена.\n"
     ]
    }
   ],
   "source": [
    "# Задайте имя репозитория на Hugging Face Hub\n",
    "# Рекомендуется формат: \"your-username/model-name-description\"\n",
    "repo_id = \"ez3nx/multilingual-sentiment-3class-lora-adapter\" # <<< ЗАМЕНИТЕ НА СВОЕ\n",
    "\n",
    "print(f\"Публикация адаптера LoRA и токенизатора в репозиторий: {repo_id}\")\n",
    "\n",
    "# Пушим адаптер (PeftModel сам знает, что пушить только адаптер)\n",
    "inference_model.push_to_hub(repo_id)\n",
    "\n",
    "# Пушим токенизатор в тот же репозиторий\n",
    "tokenizer.push_to_hub(repo_id)\n",
    "\n",
    "print(\"Публикация завершена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T16:03:33.958961Z",
     "iopub.status.busy": "2025-05-04T16:03:33.958357Z",
     "iopub.status.idle": "2025-05-04T17:18:02.648262Z",
     "shell.execute_reply": "2025-05-04T17:18:02.647412Z",
     "shell.execute_reply.started": "2025-05-04T16:03:33.958940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Начало обучения...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250504_160334-xolgqd1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/xolgqd1y' target=\"_blank\">multilingual-sentiment-analysis-lora-r32-alpha64-lr0.0003-epochs6</a></strong> to <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/xolgqd1y' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/xolgqd1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [768/768 1:14:12, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.927700</td>\n",
       "      <td>0.810142</td>\n",
       "      <td>0.642426</td>\n",
       "      <td>0.645795</td>\n",
       "      <td>0.642426</td>\n",
       "      <td>0.639911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.759400</td>\n",
       "      <td>0.735825</td>\n",
       "      <td>0.671240</td>\n",
       "      <td>0.676999</td>\n",
       "      <td>0.671240</td>\n",
       "      <td>0.670726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.711319</td>\n",
       "      <td>0.691548</td>\n",
       "      <td>0.691536</td>\n",
       "      <td>0.691548</td>\n",
       "      <td>0.691435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.604100</td>\n",
       "      <td>0.710757</td>\n",
       "      <td>0.697311</td>\n",
       "      <td>0.701033</td>\n",
       "      <td>0.697311</td>\n",
       "      <td>0.697011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.520100</td>\n",
       "      <td>0.721250</td>\n",
       "      <td>0.702525</td>\n",
       "      <td>0.703044</td>\n",
       "      <td>0.702525</td>\n",
       "      <td>0.702565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▇▇██</td></tr><tr><td>eval/f1_weighted</td><td>▁▄▇▇██</td></tr><tr><td>eval/loss</td><td>█▃▁▁▁▂</td></tr><tr><td>eval/precision_weighted</td><td>▁▅▇███</td></tr><tr><td>eval/recall_weighted</td><td>▁▄▇▇██</td></tr><tr><td>eval/runtime</td><td>▂▁▂▅█▁</td></tr><tr><td>eval/samples_per_second</td><td>▇█▇▄▁█</td></tr><tr><td>eval/steps_per_second</td><td>▇██▄▁█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▄▄▅▅▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▄▄▅▅▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▂▅▁▅█</td></tr><tr><td>train/learning_rate</td><td>▇█▆▄▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.70252</td></tr><tr><td>eval/f1_weighted</td><td>0.70257</td></tr><tr><td>eval/loss</td><td>0.72125</td></tr><tr><td>eval/precision_weighted</td><td>0.70304</td></tr><tr><td>eval/recall_weighted</td><td>0.70252</td></tr><tr><td>eval/runtime</td><td>35.4843</td></tr><tr><td>eval/samples_per_second</td><td>102.693</td></tr><tr><td>eval/steps_per_second</td><td>1.606</td></tr><tr><td>total_flos</td><td>2.580830355406219e+16</td></tr><tr><td>train/epoch</td><td>5.95906</td></tr><tr><td>train/global_step</td><td>768</td></tr><tr><td>train/grad_norm</td><td>79858.67969</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5201</td></tr><tr><td>train_loss</td><td>0.67389</td></tr><tr><td>train_runtime</td><td>4466.6633</td></tr><tr><td>train_samples_per_second</td><td>44.053</td></tr><tr><td>train_steps_per_second</td><td>0.172</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">multilingual-sentiment-analysis-lora-r32-alpha64-lr0.0003-epochs6</strong> at: <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/xolgqd1y' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/xolgqd1y</a><br> View project at: <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_160334-xolgqd1y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение завершено. Финальный адаптер LoRA сохранен в: ./sentiment_lora_finetuned_BIG_SET/final_adapter\n"
     ]
    }
   ],
   "source": [
    "# --- ЗАПУСК ОБУЧЕНИЯ ---\n",
    "print(\"\\nНачало обучения...\")\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "# --- СОХРАНЕНИЕ АДАПТЕРА ---\n",
    "# Trainer автоматически сохранит лучший адаптер в output_dir/best_model\n",
    "# Можно также сохранить явно последнюю версию адаптера:\n",
    "adapter_path = f\"{output_dir}/final_adapter\"\n",
    "peft_model_3.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path) # Сохраним и токенизатор рядом\n",
    "print(f\"Обучение завершено. Финальный адаптер LoRA сохранен в: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПОЛНЫЙ ФАЙНТЮН"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T20:09:45.665661Z",
     "iopub.status.busy": "2025-05-04T20:09:45.665069Z",
     "iopub.status.idle": "2025-05-04T20:09:45.796950Z",
     "shell.execute_reply": "2025-05-04T20:09:45.796352Z",
     "shell.execute_reply.started": "2025-05-04T20:09:45.665638Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at tabularisai/multilingual-sentiment-analysis and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([5, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка базовой модели tabularisai/multilingual-sentiment-analysis для Full Fine-Tuning...\n",
      "Модель загружена.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Value\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, # Используем стандартный коллатор\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TrainerCallback # Импортируем TrainerCallback\n",
    ")\n",
    "from transformers.integrations import WandbCallback # Если вы используете W&B\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "import wandb # Если вы используете W&B\n",
    "\n",
    "\n",
    "\n",
    "# --- Установка формата PyTorch (Как раньше) ---\n",
    "columns_to_set_format = ['input_ids', 'attention_mask', 'labels']\n",
    "tokenized_datasets.set_format(\"torch\", columns=columns_to_set_format)\n",
    "\n",
    "# --- 3. Data Collator (Используем СТАНДАРТНЫЙ) ---\n",
    "# Если ошибка типа float32 вернется, можно раскомментировать и использовать\n",
    "# ваш CustomDataCollatorWithPadding из предыдущих шагов.\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "custom_data_collator = CustomDataCollatorWithPadding(tokenizer=tokenizer) # Опционально\n",
    "\n",
    "# --- 4. Загрузка МОДЕЛИ (Без PEFT/LoRA) ---\n",
    "# Загружаем базовую модель С НУЖНЫМ КОЛИЧЕСТВОМ КЛАССОВ\n",
    "# ignore_mismatched_sizes=True автоматически заменит голову классификатора\n",
    "print(f\"Загрузка базовой модели {model_name} для Full Fine-Tuning...\")\n",
    "model_fft = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,              # <<< 3 класса\n",
    "    ignore_mismatched_sizes=True,     # <<< Заменить голову классификатора\n",
    "    id2label={i: f\"LABEL_{reverse_label_map[i]}\" for i in range(num_labels)},\n",
    "    label2id={f\"LABEL_{reverse_label_map[i]}\": i for i in range(num_labels)}\n",
    ")\n",
    "print(\"Модель загружена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T20:09:48.624295Z",
     "iopub.status.busy": "2025-05-04T20:09:48.623679Z",
     "iopub.status.idle": "2025-05-04T20:09:48.631461Z",
     "shell.execute_reply": "2025-05-04T20:09:48.630896Z",
     "shell.execute_reply.started": "2025-05-04T20:09:48.624275Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего параметров: 135,326,979\n",
      "Обучаемых параметров: 135,326,979 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Проверка количества обучаемых параметров (должно быть почти все)\n",
    "total_params = sum(p.numel() for p in model_fft.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_fft.parameters() if p.requires_grad)\n",
    "print(f\"Всего параметров: {total_params:,}\")\n",
    "print(f\"Обучаемых параметров: {trainable_params:,} ({(trainable_params/total_params)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T20:10:09.942833Z",
     "iopub.status.busy": "2025-05-04T20:10:09.942577Z",
     "iopub.status.idle": "2025-05-04T20:10:09.977074Z",
     "shell.execute_reply": "2025-05-04T20:10:09.976596Z",
     "shell.execute_reply.started": "2025-05-04T20:10:09.942814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 6. Аргументы обучения (Изменяем learning_rate и output_dir/run_name) ---\n",
    "output_dir = \"./sentiment_fft_finetuned\" # Новая директория для FFT\n",
    "logging_dir = './logs_fft'             # Новая директория логов\n",
    "\n",
    "# !!! ВАЖНО: Низкий learning rate для FFT !!!\n",
    "learning_rate = 2e-5  # Типичное значение для FFT (попробуйте 2e-5, 5e-5 если нужно)\n",
    "\n",
    "batch_size = 16 # Оставьте или измените в зависимости от памяти GPU\n",
    "num_train_epochs = 6 # FFT часто требует меньше эпох, начните с 3-5\n",
    "weight_decay = 0.0004\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Настройка W&B (если используется)\n",
    "os.environ[\"WANDB_PROJECT\"] = \"sentiment_fft_finetuning\" # Новый проект или то же имя\n",
    "model_name_short = model_name.split('/')[-1]\n",
    "run_name = f\"{model_name_short}-fft-lr{learning_rate}-epochs{num_train_epochs}\" # Имя для FFT\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    logging_dir=logging_dir,\n",
    "    learning_rate=learning_rate,          # <<< НОВЫЙ LR\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size, # Можно увеличить для ускорения оценки\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    label_names=[\"labels\"],\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    push_to_hub=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    # --- Шедулер (оставляем, полезен для FFT) ---\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.2,\n",
    "    # --- W&B ---\n",
    "    report_to=\"wandb\",                  # или \"none\"\n",
    "    run_name=run_name,\n",
    ")\n",
    "\n",
    "# --- 7. Инициализация Trainer (Передаем базовую модель) ---\n",
    "# callbacks_list = []\n",
    "# if training_args.report_to == \"wandb\":\n",
    "#      # Убедитесь, что выполнен вход через wandb.login(key=...)\n",
    "#     callbacks_list.append(WandbCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T20:10:11.953934Z",
     "iopub.status.busy": "2025-05-04T20:10:11.953677Z",
     "iopub.status.idle": "2025-05-04T21:16:37.459161Z",
     "shell.execute_reply": "2025-05-04T21:16:37.458492Z",
     "shell.execute_reply.started": "2025-05-04T20:10:11.953914Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2844417089.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Начало ПОЛНОГО Fine-Tuning...\n",
      "ПРЕДУПРЕЖДЕНИЕ: Это потребует значительно больше памяти GPU и времени, чем LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1326' max='1326' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1326/1326 1:06:02, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.951600</td>\n",
       "      <td>0.809786</td>\n",
       "      <td>0.646984</td>\n",
       "      <td>0.650579</td>\n",
       "      <td>0.646984</td>\n",
       "      <td>0.647044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.749900</td>\n",
       "      <td>0.720034</td>\n",
       "      <td>0.690159</td>\n",
       "      <td>0.701094</td>\n",
       "      <td>0.690159</td>\n",
       "      <td>0.690040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.628100</td>\n",
       "      <td>0.694548</td>\n",
       "      <td>0.713651</td>\n",
       "      <td>0.714047</td>\n",
       "      <td>0.713651</td>\n",
       "      <td>0.713595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.704709</td>\n",
       "      <td>0.716825</td>\n",
       "      <td>0.718316</td>\n",
       "      <td>0.716825</td>\n",
       "      <td>0.716716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.730151</td>\n",
       "      <td>0.716825</td>\n",
       "      <td>0.717263</td>\n",
       "      <td>0.716825</td>\n",
       "      <td>0.716870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение завершено. Лучшая модель сохранена в: ./sentiment_fft_finetuned\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_fft,                     # <<< ПЕРЕДАЕМ БАЗОВУЮ МОДЕЛЬ\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=custom_data_collator,      # <<< СТАНДАРТНЫЙ КОЛЛАТОР (или кастомный, если нужно)\n",
    "    compute_metrics=compute_metrics,\n",
    "    # callbacks=callbacks_list          # Передаем коллбэки (для W&B)\n",
    ")\n",
    "\n",
    "# --- 8. Обучение ---\n",
    "print(\"\\nНачало ПОЛНОГО Fine-Tuning...\")\n",
    "print(\"ПРЕДУПРЕЖДЕНИЕ: Это потребует значительно больше памяти GPU и времени, чем LoRA.\")\n",
    "trainer.train()\n",
    "\n",
    "# --- 9. Сохранение (Trainer сам сохранит лучшую модель) ---\n",
    "# Trainer автоматически сохранит лучший чекпоинт в training_args.output_dir\n",
    "# благодаря save_strategy=\"epoch\" и load_best_model_at_end=True.\n",
    "# Сохранять вручную не обязательно, но можно сохранить финальную модель:\n",
    "# final_model_path = f\"{output_dir}/final_model\"\n",
    "# trainer.save_model(final_model_path)\n",
    "# tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"Обучение завершено. Лучшая модель сохранена в: {output_dir}\")\n",
    "# Если использовали W&B, не забудьте wandb.finish() если нужно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 10. Инференс (Загружаем модель из папки с результатом) ---\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"\\n--- Инференс с дообученной моделью (FFT) ---\")\n",
    "# Загружаем модель, сохраненную Trainer'ом\n",
    "best_model_path = output_dir # Trainer сохраняет лучшую модель в output_dir при load_best_model_at_end=True\n",
    "# Если load_best_model_at_end=False, лучший чекпоинт будет в output_dir/checkpoint-XYZ\n",
    "\n",
    "# Вариант 1: Использование pipeline\n",
    "pipe = pipeline(\"text-classification\", model=best_model_path, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Вариант 2: Ручная загрузка и предсказание\n",
    "# inference_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
    "# inference_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "# inference_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# inference_model.eval()\n",
    "# def predict_sentiment_fft(text):\n",
    "#     inputs = inference_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#     inputs = {k: v.to(inference_model.device) for k, v in inputs.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = inference_model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#     probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "#     predicted_class_id = torch.argmax(probabilities, dim=-1).item()\n",
    "#     predicted_label = reverse_label_map[predicted_class_id]\n",
    "#     predicted_probability = probabilities[0, predicted_class_id].item()\n",
    "#     return {\"label\": predicted_label, \"score\": predicted_probability, \"label_id\": predicted_class_id}\n",
    "\n",
    "test_sentence_1 = \"Это было невероятно хорошо, я в восторге!\"\n",
    "test_sentence_2 = \"Фильм как фильм, ничего особенного.\"\n",
    "test_sentence_3 = \"Мне совсем не зашло, пустая трата времени.\"\n",
    "\n",
    "# Используем pipeline\n",
    "result1 = pipe(test_sentence_1)\n",
    "result2 = pipe(test_sentence_2)\n",
    "result3 = pipe(test_sentence_3)\n",
    "\n",
    "# # Используем ручную функцию\n",
    "# result1 = predict_sentiment_fft(test_sentence_1)\n",
    "# result2 = predict_sentiment_fft(test_sentence_2)\n",
    "# result3 = predict_sentiment_fft(test_sentence_3)\n",
    "\n",
    "\n",
    "print(f\"Предсказание для: '{test_sentence_1}' => {result1}\")\n",
    "print(f\"Предсказание для: '{test_sentence_2}' => {result2}\")\n",
    "print(f\"Предсказание для: '{test_sentence_3}' => {result3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:34:52.900804Z",
     "iopub.status.busy": "2025-05-04T13:34:52.900541Z",
     "iopub.status.idle": "2025-05-04T13:34:53.716404Z",
     "shell.execute_reply": "2025-05-04T13:34:53.715795Z",
     "shell.execute_reply.started": "2025-05-04T13:34:52.900786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T13:34:55.510994Z",
     "iopub.status.busy": "2025-05-04T13:34:55.510324Z",
     "iopub.status.idle": "2025-05-04T14:17:09.847841Z",
     "shell.execute_reply": "2025-05-04T14:17:09.847170Z",
     "shell.execute_reply.started": "2025-05-04T13:34:55.510968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Начало обучения...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250504_133455-gxkpc19i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/gxkpc19i' target=\"_blank\">multilingual-sentiment-analysis-lora-r32-alpha64-lr0.0001-epochs5</a></strong> to <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/gxkpc19i' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/gxkpc19i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1615' max='1615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1615/1615 42:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.955100</td>\n",
       "      <td>0.859144</td>\n",
       "      <td>0.617865</td>\n",
       "      <td>0.625604</td>\n",
       "      <td>0.617865</td>\n",
       "      <td>0.617284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.806600</td>\n",
       "      <td>0.782680</td>\n",
       "      <td>0.661002</td>\n",
       "      <td>0.667788</td>\n",
       "      <td>0.661002</td>\n",
       "      <td>0.660280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.769324</td>\n",
       "      <td>0.676688</td>\n",
       "      <td>0.679926</td>\n",
       "      <td>0.676688</td>\n",
       "      <td>0.675447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.664500</td>\n",
       "      <td>0.750470</td>\n",
       "      <td>0.681046</td>\n",
       "      <td>0.685300</td>\n",
       "      <td>0.681046</td>\n",
       "      <td>0.681192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.752278</td>\n",
       "      <td>0.687146</td>\n",
       "      <td>0.687830</td>\n",
       "      <td>0.687146</td>\n",
       "      <td>0.687253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_31/2557029147.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.long)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▅▇▇█</td></tr><tr><td>eval/f1_weighted</td><td>▁▅▇▇█</td></tr><tr><td>eval/loss</td><td>█▃▂▁▁</td></tr><tr><td>eval/precision_weighted</td><td>▁▆▇██</td></tr><tr><td>eval/recall_weighted</td><td>▁▅▇▇█</td></tr><tr><td>eval/runtime</td><td>▂▁▃█▁</td></tr><tr><td>eval/samples_per_second</td><td>▇█▆▁█</td></tr><tr><td>eval/steps_per_second</td><td>▇█▆▁█</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/grad_norm</td><td>▁▁▄█▇</td></tr><tr><td>train/learning_rate</td><td>█▇▅▂▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.68715</td></tr><tr><td>eval/f1_weighted</td><td>0.68725</td></tr><tr><td>eval/loss</td><td>0.75228</td></tr><tr><td>eval/precision_weighted</td><td>0.68783</td></tr><tr><td>eval/recall_weighted</td><td>0.68715</td></tr><tr><td>eval/runtime</td><td>24.0308</td></tr><tr><td>eval/samples_per_second</td><td>95.502</td></tr><tr><td>eval/steps_per_second</td><td>1.498</td></tr><tr><td>total_flos</td><td>1.3587124033643004e+16</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>1615</td></tr><tr><td>train/grad_norm</td><td>254779.71875</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.642</td></tr><tr><td>train_loss</td><td>0.75853</td></tr><tr><td>train_runtime</td><td>2531.8944</td></tr><tr><td>train_samples_per_second</td><td>40.778</td></tr><tr><td>train_steps_per_second</td><td>0.638</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">multilingual-sentiment-analysis-lora-r32-alpha64-lr0.0001-epochs5</strong> at: <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/gxkpc19i' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning/runs/gxkpc19i</a><br> View project at: <a href='https://wandb.ai/ez3nx/sentiment_lora_finetuning' target=\"_blank\">https://wandb.ai/ez3nx/sentiment_lora_finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250504_133455-gxkpc19i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение завершено. Финальный адаптер LoRA сохранен в: ./sentiment_lora_finetuned_BIG_SET/final_adapter\n"
     ]
    }
   ],
   "source": [
    "# --- ЗАПУСК ОБУЧЕНИЯ ---\n",
    "print(\"\\nНачало обучения...\")\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "# --- СОХРАНЕНИЕ АДАПТЕРА ---\n",
    "# Trainer автоматически сохранит лучший адаптер в output_dir/best_model\n",
    "# Можно также сохранить явно последнюю версию адаптера:\n",
    "adapter_path = f\"{output_dir}/final_adapter\"\n",
    "peft_model_3.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path) # Сохраним и токенизатор рядом\n",
    "print(f\"Обучение завершено. Финальный адаптер LoRA сохранен в: {adapter_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7323321,
     "sourceId": 11669213,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7325505,
     "sourceId": 11672463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7327314,
     "sourceId": 11674967,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7327902,
     "sourceId": 11675753,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
