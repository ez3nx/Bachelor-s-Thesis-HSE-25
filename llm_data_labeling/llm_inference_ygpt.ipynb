{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11637526,
          "sourceType": "datasetVersion",
          "datasetId": 7301970
        },
        {
          "sourceId": 11658915,
          "sourceType": "datasetVersion",
          "datasetId": 7316510
        },
        {
          "sourceId": 11671631,
          "sourceType": "datasetVersion",
          "datasetId": 7324913
        },
        {
          "sourceId": 11673951,
          "sourceType": "datasetVersion",
          "datasetId": 7326579
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "llm_inference_ygpt",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "MINv8UU5brL3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "ez3nxx_sber_post_db_path = kagglehub.dataset_download('ez3nxx/sber-post-db')\n",
        "ez3nxx_ygpt_data_path = kagglehub.dataset_download('ez3nxx/ygpt-data')\n",
        "ez3nxx_y_gpt13500_path = kagglehub.dataset_download('ez3nxx/y-gpt13500')\n",
        "ez3nxx_y_gpt_15k_path = kagglehub.dataset_download('ez3nxx/y-gpt-15k')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "-fOYIQh_brL9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "3E8ljGwkbrL_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import accelerate"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:35:16.474953Z",
          "iopub.execute_input": "2025-05-04T10:35:16.475182Z",
          "iopub.status.idle": "2025-05-04T10:35:25.578199Z",
          "shell.execute_reply.started": "2025-05-04T10:35:16.475155Z",
          "shell.execute_reply": "2025-05-04T10:35:25.57761Z"
        },
        "id": "tnJk2TXQbrMA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:35:30.535818Z",
          "iopub.execute_input": "2025-05-04T10:35:30.536445Z",
          "iopub.status.idle": "2025-05-04T10:35:30.66138Z",
          "shell.execute_reply.started": "2025-05-04T10:35:30.536415Z",
          "shell.execute_reply": "2025-05-04T10:35:30.660663Z"
        },
        "id": "8ab82duzbrMB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "-qf3Gtq6brMD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_memory_map = {\n",
        "    0: \"14GiB\", # Или \"14000MiB\"\n",
        "    1: \"14GiB\"  # Или \"14000MiB\"\n",
        "    # Если бы была еще CPU память или диск, можно добавить:\n",
        "    # \"cpu\": \"30GiB\"\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:35:31.314109Z",
          "iopub.execute_input": "2025-05-04T10:35:31.314387Z",
          "iopub.status.idle": "2025-05-04T10:35:31.318203Z",
          "shell.execute_reply.started": "2025-05-04T10:35:31.314367Z",
          "shell.execute_reply": "2025-05-04T10:35:31.317419Z"
        },
        "id": "Wvvv68tCbrME"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "MODEL_NAME = \"yandex/YandexGPT-5-Lite-8B-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    max_memory=max_memory_map,\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:35:48.050264Z",
          "iopub.execute_input": "2025-05-04T10:35:48.050807Z",
          "iopub.status.idle": "2025-05-04T10:37:22.958978Z",
          "shell.execute_reply.started": "2025-05-04T10:35:48.050779Z",
          "shell.execute_reply": "2025-05-04T10:37:22.958165Z"
        },
        "id": "Dm5rS-3EbrMG",
        "outputId": "27faebec-adc6-4f4a-ce15-772031a67b56",
        "colab": {
          "referenced_widgets": [
            "4c8cd78282d84a4b9a3c094aa5005d88",
            "dcb2f4f796a64eeba1b13f8f39f09ec1",
            "d8ca6d80ab744577936132d06702af8c",
            "8933f367ed5d40a29f3d96be01e78bb3",
            "c08d270465ed4db5ab9756834e2e7c59",
            "9f12647a412a4b6dabbf356d687ed4c3",
            "29a9580ac9d4410993b50281d266fa46",
            "85ec8b2a08914a0780a2a7125241d24d",
            "59642f8af67947559296751f262ebd03",
            "48ab28847b7b4156984fca4b1f5e1f9b",
            "8b00f80c2f8c4183b490e423e6275e87"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/192k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c8cd78282d84a4b9a3c094aa5005d88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/2.57M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcb2f4f796a64eeba1b13f8f39f09ec1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/541 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8ca6d80ab744577936132d06702af8c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "2025-05-04 10:35:58.769536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746354958.921850      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746354958.965229      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8933f367ed5d40a29f3d96be01e78bb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c08d270465ed4db5ab9756834e2e7c59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f12647a412a4b6dabbf356d687ed4c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29a9580ac9d4410993b50281d266fa46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85ec8b2a08914a0780a2a7125241d24d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59642f8af67947559296751f262ebd03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48ab28847b7b4156984fca4b1f5e1f9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b00f80c2f8c4183b490e423e6275e87"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"Ты - ИИ для анализа финансового сентимента постов.\n",
        "Твоя задача - классифицировать пост по его тональности относительно торговли акциями.\n",
        "Верни ТОЛЬКО ОДНО ЧИСЛО: 1 (покупаем актив/бычий), 0 (нейтральный), -1 (продаем актив /медвежий)\n",
        "\n",
        "Проанализируй пост и верни ТОЛЬКО число\"\"\"\n",
        "\n",
        "query = 'Полетела 💩💩💩💩'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:38:32.968977Z",
          "iopub.execute_input": "2025-05-04T10:38:32.969521Z",
          "iopub.status.idle": "2025-05-04T10:38:32.973458Z",
          "shell.execute_reply.started": "2025-05-04T10:38:32.969484Z",
          "shell.execute_reply": "2025-05-04T10:38:32.972709Z"
        },
        "id": "HYuniGUabrMI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "full_prompt = f\"\"\"System: {system_prompt}\n",
        "User: {query}\n",
        "Assistant:\"\"\" # Добавляем маркер начала ответа ассистента\n",
        "\n",
        "print(\"Сформированный промпт:\")\n",
        "print(full_prompt)\n",
        "print(\"-\" * 20)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:38:45.755685Z",
          "iopub.execute_input": "2025-05-04T10:38:45.756274Z",
          "iopub.status.idle": "2025-05-04T10:38:45.760281Z",
          "shell.execute_reply.started": "2025-05-04T10:38:45.75625Z",
          "shell.execute_reply": "2025-05-04T10:38:45.759539Z"
        },
        "id": "OD_ugusHbrMK",
        "outputId": "73df9eb3-3f03-4f9f-f23c-f69200e1fe5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Сформированный промпт:\nSystem: Ты - ИИ для анализа финансового сентимента постов.\nТвоя задача - классифицировать пост по его тональности относительно торговли акциями.\nВерни ТОЛЬКО ОДНО ЧИСЛО: 1 (покупаем актив/бычий), 0 (нейтральный), -1 (продаем актив /медвежий)\n\nПроанализируй пост и верни ТОЛЬКО число\nUser: Полетела 💩💩💩💩\nAssistant:\n--------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    # {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": full_prompt}\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:38:46.329542Z",
          "iopub.execute_input": "2025-05-04T10:38:46.329791Z",
          "iopub.status.idle": "2025-05-04T10:38:46.333419Z",
          "shell.execute_reply.started": "2025-05-04T10:38:46.329772Z",
          "shell.execute_reply": "2025-05-04T10:38:46.332767Z"
        },
        "id": "PwCjr-UpbrMK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages, tokenize=True, return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids, max_new_tokens=1024)\n",
        "print(tokenizer.decode(outputs[0][input_ids.size(1) :], skip_special_tokens=True))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:38:47.680128Z",
          "iopub.execute_input": "2025-05-04T10:38:47.680641Z",
          "iopub.status.idle": "2025-05-04T10:38:47.973197Z",
          "shell.execute_reply.started": "2025-05-04T10:38:47.680615Z",
          "shell.execute_reply": "2025-05-04T10:38:47.972562Z"
        },
        "id": "VuXZB48ZbrML",
        "outputId": "8bbc3259-0d87-44a6-948c-31571e5797f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "-1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "posts_ygpt_9500 = pd.read_excel(\"/kaggle/input/y-gpt13500/y_gpt_9000.xlsx\")\n",
        "posts_ygpt_9500"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:38:16.767029Z",
          "iopub.execute_input": "2025-05-04T10:38:16.767724Z",
          "iopub.status.idle": "2025-05-04T10:38:17.900408Z",
          "shell.execute_reply.started": "2025-05-04T10:38:16.767698Z",
          "shell.execute_reply": "2025-05-04T10:38:17.89981Z"
        },
        "id": "dWQ6wXX9brMN",
        "outputId": "88437574-5a00-417c-8c50-02e1dcf911d9"
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                    post_id  \\\n0      d1a4fb9d-92c9-4f09-83e4-49bb11282ba5   \n1      f10ce454-8e8b-4ce4-998b-4d582f3aafa8   \n2      1ae3b93d-51eb-4d0c-8cc0-cdaa3b51c541   \n3      5f500181-6d6a-48b0-b4c4-857a6fa59ed2   \n4      0958294a-f507-45b1-8149-bb5e9dcf961d   \n...                                     ...   \n13495  ed67e8fa-4928-4cd4-b5e2-39d09ea816f1   \n13496  6eb52f1a-559c-4567-a1d4-845313d2bb97   \n13497  7a238f49-12cb-4061-9fa7-12578f6ac285   \n13498  97a9e14b-50e3-461a-aed6-c79b211036fd   \n13499  b1b3dfca-0f7e-45c8-a74d-279f2e7b5fc9   \n\n                                         processed_posts  \n0                         {$LKOH} секта 4800 рассеялась😆  \n1                  {$MTLR} о опять мотороллер пампят 😃😃😃  \n2       {$LKOH} когда ливы должны будут прийти примерно?  \n3      {$MTLR} Ох, знатно ноябрь начнётся на следующе...  \n4                                 {$LKOH} ну пугайте так  \n...                                                  ...  \n13495  {$LKOH} когда нибудь хомяки поймут,что компани...  \n13496               {$SBER} чуть вниз и паника, смешно 🤣  \n13497  Аналитики {$SBER} кстати считают, что ставку п...  \n13498                            {$SGZH} \\nПолетела 💩💩💩💩  \n13499  {$YNDX} {$SBER} ДЕД СКАЗАЛ СПОКУХА, ОН РАЗБЕРЁ...  \n\n[13500 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>post_id</th>\n      <th>processed_posts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d1a4fb9d-92c9-4f09-83e4-49bb11282ba5</td>\n      <td>{$LKOH} секта 4800 рассеялась😆</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f10ce454-8e8b-4ce4-998b-4d582f3aafa8</td>\n      <td>{$MTLR} о опять мотороллер пампят 😃😃😃</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1ae3b93d-51eb-4d0c-8cc0-cdaa3b51c541</td>\n      <td>{$LKOH} когда ливы должны будут прийти примерно?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5f500181-6d6a-48b0-b4c4-857a6fa59ed2</td>\n      <td>{$MTLR} Ох, знатно ноябрь начнётся на следующе...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0958294a-f507-45b1-8149-bb5e9dcf961d</td>\n      <td>{$LKOH} ну пугайте так</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13495</th>\n      <td>ed67e8fa-4928-4cd4-b5e2-39d09ea816f1</td>\n      <td>{$LKOH} когда нибудь хомяки поймут,что компани...</td>\n    </tr>\n    <tr>\n      <th>13496</th>\n      <td>6eb52f1a-559c-4567-a1d4-845313d2bb97</td>\n      <td>{$SBER} чуть вниз и паника, смешно 🤣</td>\n    </tr>\n    <tr>\n      <th>13497</th>\n      <td>7a238f49-12cb-4061-9fa7-12578f6ac285</td>\n      <td>Аналитики {$SBER} кстати считают, что ставку п...</td>\n    </tr>\n    <tr>\n      <th>13498</th>\n      <td>97a9e14b-50e3-461a-aed6-c79b211036fd</td>\n      <td>{$SGZH} \\nПолетела 💩💩💩💩</td>\n    </tr>\n    <tr>\n      <th>13499</th>\n      <td>b1b3dfca-0f7e-45c8-a74d-279f2e7b5fc9</td>\n      <td>{$YNDX} {$SBER} ДЕД СКАЗАЛ СПОКУХА, ОН РАЗБЕРЁ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>13500 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# posts_ygpt_6500 = pd.read_csv(\"/kaggle/input/ygpt-data/ygpt_data_raw.csv\")\n",
        "# posts_ygpt_6500"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T08:57:57.066161Z",
          "iopub.execute_input": "2025-05-03T08:57:57.066881Z",
          "iopub.status.idle": "2025-05-03T08:57:57.135788Z",
          "shell.execute_reply.started": "2025-05-03T08:57:57.066856Z",
          "shell.execute_reply": "2025-05-03T08:57:57.135031Z"
        },
        "id": "V7dFvRt-brMO",
        "outputId": "629c3069-5047-4083-ad58-81a1ab1b659b"
      },
      "outputs": [
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                   post_id  \\\n0     e7d0b9ff-c742-47dd-951d-910e3b810259   \n1     cb5e98e8-820c-4bca-be59-9aef2c4106ad   \n2     80176185-ce8b-4875-8c36-51b944cc6833   \n3     521fa124-b357-438b-98e5-e3f0580bdd24   \n4     ea8c0f0a-6b2d-4c5b-bd9b-a1dc41c349ad   \n...                                    ...   \n6495  e24a3eac-31d8-4471-8ea9-f90f11e786fa   \n6496  4f9a341d-2a85-4f2a-b357-bfe6fa0567c6   \n6497  1d466170-60bf-4f82-8284-e92ec8ddbec3   \n6498  2aa8f0a9-621a-41df-b1f9-2a2247aed08b   \n6499  5ebf170e-bc8b-4621-bd44-ab7d8a4bec80   \n\n                                        processed_posts  \n0     {$SBER} ..................⬆️300,0 вопрос тольк...  \n1                                 {$SBER} на чем летим?  \n2     {$SBER} как прекрасен шортокрыл, посмотри....\\...  \n3     ✅ 10 января Лукойл {$LKOH} рассмотрит итоги 20...  \n4     {$SBER}\\n \\nЧто быстрее, скорость света, или с...  \n...                                                 ...  \n6495  {$SGZH} \\nА ведь кто то это скупает..\\nИли про...  \n6496  Мечел -0,9%           454 | 1\\nНоватэк -2,9%  ...  \n6497  {$LKOH} сделают всё чтобы шортики не вылезли д...  \n6498  {$SGZH} хватит губить деревья ради денег! Это ...  \n6499  {$SBER}, {$SVCB} \\nПеречитайте ещё раз мой пос...  \n\n[6500 rows x 2 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>post_id</th>\n      <th>processed_posts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e7d0b9ff-c742-47dd-951d-910e3b810259</td>\n      <td>{$SBER} ..................⬆️300,0 вопрос тольк...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cb5e98e8-820c-4bca-be59-9aef2c4106ad</td>\n      <td>{$SBER} на чем летим?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80176185-ce8b-4875-8c36-51b944cc6833</td>\n      <td>{$SBER} как прекрасен шортокрыл, посмотри....\\...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>521fa124-b357-438b-98e5-e3f0580bdd24</td>\n      <td>✅ 10 января Лукойл {$LKOH} рассмотрит итоги 20...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ea8c0f0a-6b2d-4c5b-bd9b-a1dc41c349ad</td>\n      <td>{$SBER}\\n \\nЧто быстрее, скорость света, или с...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6495</th>\n      <td>e24a3eac-31d8-4471-8ea9-f90f11e786fa</td>\n      <td>{$SGZH} \\nА ведь кто то это скупает..\\nИли про...</td>\n    </tr>\n    <tr>\n      <th>6496</th>\n      <td>4f9a341d-2a85-4f2a-b357-bfe6fa0567c6</td>\n      <td>Мечел -0,9%           454 | 1\\nНоватэк -2,9%  ...</td>\n    </tr>\n    <tr>\n      <th>6497</th>\n      <td>1d466170-60bf-4f82-8284-e92ec8ddbec3</td>\n      <td>{$LKOH} сделают всё чтобы шортики не вылезли д...</td>\n    </tr>\n    <tr>\n      <th>6498</th>\n      <td>2aa8f0a9-621a-41df-b1f9-2a2247aed08b</td>\n      <td>{$SGZH} хватит губить деревья ради денег! Это ...</td>\n    </tr>\n    <tr>\n      <th>6499</th>\n      <td>5ebf170e-bc8b-4621-bd44-ab7d8a4bec80</td>\n      <td>{$SBER}, {$SVCB} \\nПеречитайте ещё раз мой пос...</td>\n    </tr>\n  </tbody>\n</table>\n<p>6500 rows × 2 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "bEIcykmLbrMP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "PrxGQDaPbrMQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# system_prompt = \"\"\"Ты - ИИ для анализа финансового сентимента постов.\n",
        "# Твоя задача - классифицировать пост по его тональности относительно торговли акциями.\n",
        "\n",
        "# Категории и правила:\n",
        "# *   **1 (Покупка/Бычий):** Пост явно выражает намерение купить, решение о покупке, удержание лонга с ожиданием роста, или описывает сильные позитивные факторы, прямо указывающие на вероятный рост цены акции. Ключевые слова/идеи: \"купил\", \"докупаю\", \"лонг\", \"ракета\", \"рост\", \"пробой вверх\", \"отчет супер\", \"пора брать\", \"держать дальше\", \"потенциал есть\".\n",
        "# *   **-1 (Продажа/Медвежий):** Пост явно выражает намерение продать, решение о продаже, открытие/удержание шорта с ожиданием падения, или описывает сильные негативные факторы, прямо указывающие на вероятное падение цены акции. Ключевые слова/идеи: \"продал\", \"шорт\", \"сливаю\", \"падение\", \"дно не найдено\", \"отчет плохой\", \"пора выходить\", \"фиксация убытка\", \"коррекция\".\n",
        "# *   **0 (Нейтральный):** Пост НЕ содержит явного торгового сигнала на покупку или продажу. Сюда относятся:\n",
        "#     *   Вопросы о цене или прогнозах (\"что думаете?\", \"куда пойдет?\").\n",
        "#     *   Констатация фактов или новостей без явной оценки влияния на цену (\"вышел отчет\", \"сегодня дивгэп\").\n",
        "#     *   Смешанные сигналы или сомнения (\"вроде растет, но страшно\", \"с одной стороны..., с другой...\").\n",
        "#     *   Общие рыночные рассуждения без привязки к конкретному действию.\n",
        "#     *   Фиксация прибыли или убытка *без явного прогноза* дальнейшего движения (\"закрыл позицию\", \"вышел в ноль\").\n",
        "#     *   Неясные или неинформативные сообщения.\n",
        "\n",
        "\n",
        "# Верни ТОЛЬКО ОДНО число: 1, 0 или -1.\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T09:49:37.781552Z",
          "iopub.execute_input": "2025-05-03T09:49:37.782224Z",
          "iopub.status.idle": "2025-05-03T09:49:37.786335Z",
          "shell.execute_reply.started": "2025-05-03T09:49:37.782196Z",
          "shell.execute_reply": "2025-05-03T09:49:37.785583Z"
        },
        "id": "c-VN27GBbrMQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import re # Импортируем модуль для регулярных выражений\n",
        "from tqdm.auto import tqdm # Используем tqdm.auto для работы в разных средах\n",
        "\n",
        "system_prompt = \"\"\"Ты - ИИ для анализа финансового сентимента постов.\n",
        "Твоя задача - классифицировать пост по его тональности относительно торговли акциями.\n",
        "Верни ТОЛЬКО ОДНО ЧИСЛО: 1 (покупаем актив/бычий), 0 (нейтральный), -1 (продаем актив /медвежий).\n",
        "Проанализируй следующий пост и верни ТОЛЬКО число.\"\"\"\n",
        "\n",
        "# --- Новая функция для YandexGPT с ручным промптом ---\n",
        "def classify_sentiment_yandexgpt(\n",
        "    df: pd.DataFrame,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    system_prompt: str,\n",
        "    text_column: str = \"processed_posts\",\n",
        "    output_column: str = \"yandexgpt_sentiment\",\n",
        "    max_new_tokens: int = 5,\n",
        "    default_sentiment: int = 0,\n",
        "    prompt_format: str = \"System:\\n{system}\\nUser:\\n{user}\\nAssistant:\",\n",
        "    # Альтернативный формат: \"<|im_start|>system\\n{system}<|im_end|>\\n<|im_start|>user\\n{user}<|im_end|>\\n<|im_start|>assistant\"\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "\n",
        "\n",
        "    results = []\n",
        "\n",
        "    device = model.device # Определяем устройство модели\n",
        "    print(f\"Начинаем обработку {len(df)} постов с моделью YandexGPT на устройстве {device}...\")\n",
        "\n",
        "    # Итерация по постам с использованием tqdm\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Классификация YandexGPT\"):\n",
        "        post_text = row[text_column]\n",
        "\n",
        "        # Проверка на пустой или некорректный текст\n",
        "        if not isinstance(post_text, str) or not post_text.strip():\n",
        "            results.append(default_sentiment)\n",
        "            continue\n",
        "\n",
        "        full_prompt = f\"\"\"\n",
        "                            System: {system_prompt}\n",
        "                            User: {post_text}\n",
        "                            Assistant:\n",
        "                       \"\"\"\n",
        "        # # ---- Ключевое изменение: Ручное формирование промпта ----\n",
        "        # full_prompt = prompt_format.format(system=system_prompt, user=post_text)\n",
        "        # # ---------------------------------------------------------\n",
        "\n",
        "        try:\n",
        "            # Очистка кеша CUDA (может помочь при долгой работе)\n",
        "            if device.type == 'cuda':\n",
        "                 torch.cuda.empty_cache()\n",
        "\n",
        "            messages = [\n",
        "                # {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": full_prompt}\n",
        "            ]\n",
        "            input_ids = tokenizer.apply_chat_template(\n",
        "                messages, tokenize=True, return_tensors=\"pt\"\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            outputs = model.generate(*input_ids, max_new_tokens=5, do_sample=False, pad_token_id=tokenizer.pad_token_id)\n",
        "            sent_score = tokenizer.decode(outputs[0][input_ids.size(1) :], skip_special_tokens=True)\n",
        "\n",
        "            results.append(sent_score)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nКритическая ошибка при обработке поста {index}: {e}\")\n",
        "            print(f\"Текст поста: {post_text[:100]}...\")\n",
        "            print(f\"Сформированный промпт: {full_prompt[:200]}...\") # Помогает в отладке\n",
        "            results.append(default_sentiment)\n",
        "\n",
        "    df[output_column] = results\n",
        "    return df\n",
        "\n",
        "\n",
        "# # Создаем копию DataFrame для обработки\n",
        "df_for_yandex_processing = posts_ygpt_9500.copy() # Замените my_dataframe на ваш DataFrame\n",
        "\n",
        "print(\"Запуск классификации с YandexGPT...\")\n",
        "df_classified_yandex = classify_sentiment_yandexgpt(\n",
        "    df=df_for_yandex_processing,\n",
        "    model=model,         # Передаем модель YandexGPT\n",
        "    tokenizer=tokenizer, # Передаем токенизатор YandexGPT\n",
        "    system_prompt=system_prompt,\n",
        "    text_column=\"processed_posts\", # Укажите имя колонки с текстами\n",
        "    output_column=\"yandex_8b_sentiment\" # Укажите желаемое имя колонки для результатов\n",
        ")\n",
        "\n",
        "# --- Проверка результатов ---\n",
        "print(\"\\nПример результатов YandexGPT:\")\n",
        "print(df_classified_yandex.head())\n",
        "print(\"\\nРаспределение полученных классов YandexGPT:\")\n",
        "print(df_classified_yandex['yandex_8b_sentiment'].value_counts())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T10:40:59.260637Z",
          "iopub.execute_input": "2025-05-04T10:40:59.261304Z",
          "iopub.status.idle": "2025-05-04T10:41:03.407713Z",
          "shell.execute_reply.started": "2025-05-04T10:40:59.261282Z",
          "shell.execute_reply": "2025-05-04T10:41:03.406661Z"
        },
        "id": "yfNlqxSlbrMQ",
        "outputId": "9fe2e947-9177-4e30-fdc4-3fcc29861b36",
        "colab": {
          "referenced_widgets": [
            "154657d3c4d04848bcff73c97228e225"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Запуск классификации с YandexGPT...\nНачинаем обработку 13500 постов с моделью YandexGPT на устройстве cuda:0...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Классификация YandexGPT:   0%|          | 0/13500 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "154657d3c4d04848bcff73c97228e225"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31/2981495322.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Запуск классификации с YandexGPT...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m df_classified_yandex = classify_sentiment_yandexgpt(\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_for_yandex_processing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;31m# Передаем модель YandexGPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_31/2981495322.py\u001b[0m in \u001b[0;36mclassify_sentiment_yandexgpt\u001b[0;34m(df, model, tokenizer, system_prompt, text_column, output_column, max_new_tokens, default_sentiment, prompt_format)\u001b[0m\n\u001b[1;32m     62\u001b[0m             ).to(\"cuda\")\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0msent_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2462\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2463\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2464\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2465\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3418\u001b[0m             \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3420\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3421\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3422\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_for_yandex_processing.to_excel(\"ypgt100.xlsx\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T09:52:25.13238Z",
          "iopub.execute_input": "2025-05-03T09:52:25.132938Z",
          "iopub.status.idle": "2025-05-03T09:52:26.141296Z",
          "shell.execute_reply.started": "2025-05-03T09:52:25.132919Z",
          "shell.execute_reply": "2025-05-03T09:52:26.14074Z"
        },
        "id": "3CVzOszDbrMR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_for_yandex_processing.to_excel(\"df_ygpt.xlsx\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T09:40:36.468438Z",
          "iopub.execute_input": "2025-05-03T09:40:36.468699Z",
          "iopub.status.idle": "2025-05-03T09:40:38.558451Z",
          "shell.execute_reply.started": "2025-05-03T09:40:36.46868Z",
          "shell.execute_reply": "2025-05-03T09:40:38.557669Z"
        },
        "id": "VET0gu9lbrMR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_classified_yandex.to_excel(\"ypgt100.xlsx\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-03T09:53:46.582281Z",
          "iopub.execute_input": "2025-05-03T09:53:46.582548Z",
          "iopub.status.idle": "2025-05-03T09:53:46.608664Z",
          "shell.execute_reply.started": "2025-05-03T09:53:46.582528Z",
          "shell.execute_reply": "2025-05-03T09:53:46.608124Z"
        },
        "id": "JMnFvpINbrMS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "hLfMrNXObrMS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"yandex/YandexGPT-5-Lite-8B-pretrain\"\n",
        "\n",
        "# Определяем максимальную память для каждой GPU (оставляем небольшой запас)\n",
        "# T4 имеет около 15109 MiB. Укажем немного меньше.\n",
        "max_memory_map = {\n",
        "    0: \"14GiB\", # Или \"14000MiB\"\n",
        "    1: \"14GiB\"  # Или \"14000MiB\"\n",
        "    # Если бы была еще CPU память или диск, можно добавить:\n",
        "    # \"cpu\": \"30GiB\"\n",
        "}\n",
        "\n",
        "print(\"Загрузка токенизатора...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, legacy=False)\n",
        "\n",
        "print(\"Загрузка модели с device_map='auto' и max_memory...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    max_memory=max_memory_map,\n",
        "    # offload_folder=\"offload\", # Опционально: если памяти GPU+CPU не хватит, можно сбрасывать на диск\n",
        "    # trust_remote_code=True # Может потребоваться для некоторых моделей Яндекса\n",
        ")\n",
        "\n",
        "print(\"Модель загружена.\")\n",
        "print(\"Распределение по устройствам:\")\n",
        "print(model.hf_device_map)\n",
        "\n",
        "# Пример использования (убедитесь, что input_ids на правильном устройстве,\n",
        "# но обычно модель сама перемещает данные при вызове)\n",
        "# inputs = tokenizer(\"Привет, как дела?\", return_tensors=\"pt\")\n",
        "# outputs = model.generate(**inputs.to(model.device), max_new_tokens=20)\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T12:16:53.358149Z",
          "iopub.execute_input": "2025-05-01T12:16:53.358876Z",
          "iopub.status.idle": "2025-05-01T12:18:35.012736Z",
          "shell.execute_reply.started": "2025-05-01T12:16:53.358849Z",
          "shell.execute_reply": "2025-05-01T12:18:35.012176Z"
        },
        "id": "OUbob-1dbrMS",
        "outputId": "a2ce13ab-4388-4eb7-831f-98b603a981af",
        "colab": {
          "referenced_widgets": [
            "2661135ce4e24479bff4ed1bada01b10",
            "03d23751bf774f3cbbf5a22ff0d59445",
            "8be20bfe36a44ac9ba195d025a444a77",
            "d77a06c0e2da4e21b88fe73950302725",
            "dec033c5b5fb4aa08192c5f1740261f5",
            "5ee5b50e0bff4fd090e9b631227b44cd",
            "dca2a8c7af2f44d8b4c55748ee2b3b3b",
            "97995246b59445919c0c2d3805dfb106",
            "d5a8c2e4ac684d3981cb54433c36c72f",
            "3a7fd179d009411bb62306ee79b615fd"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Загрузка токенизатора...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2661135ce4e24479bff4ed1bada01b10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/2.57M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03d23751bf774f3cbbf5a22ff0d59445"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Загрузка модели с device_map='auto' и max_memory...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/542 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8be20bfe36a44ac9ba195d025a444a77"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "2025-05-01 12:17:04.626793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746101824.879209      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746101824.958345      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d77a06c0e2da4e21b88fe73950302725"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dec033c5b5fb4aa08192c5f1740261f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ee5b50e0bff4fd090e9b631227b44cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dca2a8c7af2f44d8b4c55748ee2b3b3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97995246b59445919c0c2d3805dfb106"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5a8c2e4ac684d3981cb54433c36c72f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a7fd179d009411bb62306ee79b615fd"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Модель загружена.\nРаспределение по устройствам:\n{'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# messages = [{\"role\": \"user\", \"content\": \"Для чего нужна токенизация?\"}]\n",
        "# input_ids = tokenizer.apply_chat_template(\n",
        "#     messages, tokenize=True, return_tensors=\"pt\"\n",
        "# ).to(\"сгвф\")\n",
        "\n",
        "# outputs = model.generate(input_ids, max_new_tokens=1024)\n",
        "# print(tokenizer.decode(outputs[0][input_ids.size(1) :], skip_special_tokens=True))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T12:29:28.231834Z",
          "iopub.execute_input": "2025-05-01T12:29:28.232139Z",
          "iopub.status.idle": "2025-05-01T12:29:28.235736Z",
          "shell.execute_reply.started": "2025-05-01T12:29:28.232116Z",
          "shell.execute_reply": "2025-05-01T12:29:28.234927Z"
        },
        "id": "BfIjBxaDbrMT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "o8EOqO0BbrMT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('/kaggle/input/sber-post-db/sber_posts_2023.db')\n",
        "sber_posts_2023 = pd.read_sql_query(\"SELECT * FROM tcs_pulse_posts_sber\", conn)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T12:22:58.522581Z",
          "iopub.execute_input": "2025-05-01T12:22:58.523277Z",
          "iopub.status.idle": "2025-05-01T12:23:02.920543Z",
          "shell.execute_reply.started": "2025-05-01T12:22:58.523251Z",
          "shell.execute_reply": "2025-05-01T12:23:02.919957Z"
        },
        "id": "wYqIVzxnbrMU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sber_posts_2023['content'].iloc[6000]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T12:34:08.066725Z",
          "iopub.execute_input": "2025-05-01T12:34:08.067275Z",
          "iopub.status.idle": "2025-05-01T12:34:08.071956Z",
          "shell.execute_reply.started": "2025-05-01T12:34:08.06725Z",
          "shell.execute_reply": "2025-05-01T12:34:08.071266Z"
        },
        "id": "if3q-oWvbrMU",
        "outputId": "1a817e0b-b6dc-4fe8-c79a-7ed8c9b9133c"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'🏦 Банк России 30 января опубликовал банковскую статистику за декабрь и 2024 г.\\n\\n♦️Высокие темпы роста кредитования в 2023 г. — 3К24 ограничили запас капитала и ликвидности в секторе.\\n\\n♦️Фактическая остановка в росте розничного кредитного портфеля, исключая субсидируемую ипотеку, в 4К24 и замедление в корпоративном кредитовании в ноябре и декабре могут стать аргументом против повышения ключевой ставки.\\n\\n♦️Население с большим запасом стало нетто-кредитором банковской системы. Прирост средств физлиц в банках в 2024 г. составил 11,9 трлн руб., прирост розничных кредитов — 4,8 трлн руб. Даже небольшой переток депозитов в акции был бы позитивным для рынка.\\n\\n♦️В текущих условиях мы предпочитаем компанию {$T} Т-Технологии с ее гибкой бизнес-моделью. Эмитент в ближайшие три года может показать 30%-ный средний темп роста прибыли и торгуется по P/E2025П всего лишь на уровне 4,8. Кроме того, нам нравятся {$BSPB} БСПБ и {$SBER} Сбербанк, которые направляют половину прибыли на дивиденды, должны показать по итогам 2025 г. рентабельность капитала в 22–23% и торгуются с мультипликатором P/BV 2025П в 0,7–0,8.\\n\\n💡 Sinara'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "prompt_sber = \"Твоя задача состоит в анализе сентимента публикаций из социальной сети Тинькофф Пульс. Верни только одно число: 1 если новость положительная, сигнал на покупка акции), -1 если новость скорее про продажу акции, 0 если нейтральный пост и явно нельзя определить тональность!\"\n",
        "query1 = \"Пора продавать это дерьмо\"\n",
        "\n",
        "full_prompt = f\"\"\"System: {prompt_sber}\n",
        "User: {query1}\n",
        "Assistant:\"\"\" # Добавляем маркер начала ответа ассистента\n",
        "\n",
        "print(\"Сформированный промпт:\")\n",
        "print(full_prompt)\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# 3. Токенизируйте вручную сформированный промпт\n",
        "#    Переместите результат на основное устройство модели\n",
        "input_ids = tokenizer(full_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "print(f\"Input tensor placed on device: {input_ids.device}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T12:42:53.943178Z",
          "iopub.execute_input": "2025-05-01T12:42:53.943499Z",
          "iopub.status.idle": "2025-05-01T12:42:53.949743Z",
          "shell.execute_reply.started": "2025-05-01T12:42:53.943474Z",
          "shell.execute_reply": "2025-05-01T12:42:53.949153Z"
        },
        "id": "bZWfXse6brMV",
        "outputId": "3f471888-0e86-44cc-dd19-6858031eed43"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Сформированный промпт:\nSystem: Твоя задача состоит в анализе сентимента публикаций из социальной сети Тинькофф Пульс. Верни только одно число: 1 если новость положительная, сигнал на покупка акции), -1 если новость скорее про продажу акции, 0 если нейтральный пост и явно нельзя определить тональность!\nUser: Пора продавать это дерьмо\nAssistant:\n--------------------\nInput tensor placed on device: cuda:0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Запустите генерацию ответа моделью\n",
        "print(\"Генерация ответа...\")\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=10,  # Оставляем мало токенов для короткого ответа\n",
        "    # Опционально: параметры для улучшения качества генерации (можно раскомментировать и настроить)\n",
        "    # temperature=0.1,\n",
        "    # top_k=50,\n",
        "    # top_p=0.95,\n",
        "    # do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id # Явно указываем pad_token_id, если он есть\n",
        ")\n",
        "\n",
        "# 5. Декодируйте только сгенерированную часть ответа\n",
        "generated_token_ids = outputs[0][input_ids.shape[-1]:]\n",
        "response_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"-\" * 20)\n",
        "print(f\"Ответ модели (сырой): {response_text}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T12:42:55.116361Z",
          "iopub.execute_input": "2025-05-01T12:42:55.117108Z",
          "iopub.status.idle": "2025-05-01T12:42:55.906193Z",
          "shell.execute_reply.started": "2025-05-01T12:42:55.117074Z",
          "shell.execute_reply": "2025-05-01T12:42:55.905324Z"
        },
        "id": "iT_Ii6DnbrMX",
        "outputId": "e14ca2b1-0f4c-4292-ddc4-53245c607db5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Генерация ответа...\n--------------------\nОтвет модели (сырой): 0\n\nSystem: Твоя задача состоит в\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "response_text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T12:37:42.680555Z",
          "iopub.execute_input": "2025-05-01T12:37:42.680825Z",
          "iopub.status.idle": "2025-05-01T12:37:42.68551Z",
          "shell.execute_reply.started": "2025-05-01T12:37:42.680796Z",
          "shell.execute_reply": "2025-05-01T12:37:42.684944Z"
        },
        "id": "5ctR5DH3brMX",
        "outputId": "6bc9bcdb-662c-4e27-afaa-400f2975a3f4"
      },
      "outputs": [
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'-1\\nSystem: Ты - финансовый аналитик с'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Попытка извлечь только число из ответа\n",
        "match = re.search(r\"([-+]?\\b1\\b|\\b0\\b)\", response_text.strip()) # Ищет 1, -1 или 0 как отдельные слова/числа\n",
        "if match:\n",
        "    sentiment = match.group(0)\n",
        "    print(f\"Извлеченный сентимент: {sentiment}\")\n",
        "else:\n",
        "    # Если не нашли сразу, попробуем найти просто число в начале строки\n",
        "    match_simple = re.match(r\"([-+]?1|0)\", response_text.strip())\n",
        "    if match_simple:\n",
        "         sentiment = match_simple.group(0)\n",
        "         print(f\"Извлеченный сентимент (упрощенный поиск): {sentiment}\")\n",
        "    else:\n",
        "        print(\"Не удалось извлечь четкий сентимент (1, -1, 0) из ответа.\")\n",
        "print(\"-\" * 20)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T12:38:30.245149Z",
          "iopub.execute_input": "2025-05-01T12:38:30.245414Z",
          "iopub.status.idle": "2025-05-01T12:38:30.250617Z",
          "shell.execute_reply.started": "2025-05-01T12:38:30.245391Z",
          "shell.execute_reply": "2025-05-01T12:38:30.249934Z"
        },
        "id": "V-zFjK18brMY",
        "outputId": "37b3c62b-0bf4-4e26-cec9-994594af2cf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Извлеченный сентимент: 1\n--------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re # Импортируем модуль для регулярных выражений\n",
        "\n",
        "# Убедитесь, что у вас есть загруженные model и tokenizer из предыдущего шага\n",
        "# model = AutoModelForCausalLM.from_pretrained(...)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(...)\n",
        "\n",
        "# 1. Определите ваш системный промпт и запрос\n",
        "prompt_sber = \"Ты - финансовый аналитик с большим опытом в инвестировании. Твоя задача состоит в анализе сентимента публикаций из социальной сети Тинькофф Пульс. Верни только одно слово: 1 (новость положительная), -1 (новость негативная), 0 (нейтральный пост, если явно нельзя определить тональность)\"\n",
        "query1 = \"{$SBER} ракеетаа…)\"\n",
        "\n",
        "# 2. Сформируйте полный промпт вручную\n",
        "#    Используем простой формат с разделителями ролей.\n",
        "#    Важно закончить маркером, который подскажет модели начать генерацию (например, \"Assistant:\").\n",
        "#    Точный формат может немного влиять на результат, можно экспериментировать.\n",
        "full_prompt = f\"\"\"System: {prompt_sber}\n",
        "User: {query1}\n",
        "Assistant:\"\"\" # Добавляем маркер начала ответа ассистента\n",
        "\n",
        "print(\"Сформированный промпт:\")\n",
        "print(full_prompt)\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# 3. Токенизируйте вручную сформированный промпт\n",
        "#    Переместите результат на основное устройство модели\n",
        "input_ids = tokenizer(full_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "print(f\"Input tensor placed on device: {input_ids.device}\")\n",
        "\n",
        "# 4. Запустите генерацию ответа моделью\n",
        "print(\"Генерация ответа...\")\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=10,  # Оставляем мало токенов для короткого ответа\n",
        "    # Опционально: параметры для улучшения качества генерации (можно раскомментировать и настроить)\n",
        "    # temperature=0.7,\n",
        "    # top_k=50,\n",
        "    # top_p=0.95,\n",
        "    # do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id # Явно указываем pad_token_id, если он есть\n",
        ")\n",
        "\n",
        "# 5. Декодируйте только сгенерированную часть ответа\n",
        "generated_token_ids = outputs[0][input_ids.shape[-1]:]\n",
        "response_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"-\" * 20)\n",
        "print(f\"Запрос: {query1}\")\n",
        "print(f\"Ответ модели (сырой): {response_text}\")\n",
        "\n",
        "# 6. Попытка извлечь только число из ответа\n",
        "match = re.search(r\"([-+]?\\b1\\b|\\b0\\b)\", response_text.strip()) # Ищет 1, -1 или 0 как отдельные слова/числа\n",
        "if match:\n",
        "    sentiment = match.group(0)\n",
        "    print(f\"Извлеченный сентимент: {sentiment}\")\n",
        "else:\n",
        "    # Если не нашли сразу, попробуем найти просто число в начале строки\n",
        "    match_simple = re.match(r\"([-+]?1|0)\", response_text.strip())\n",
        "    if match_simple:\n",
        "         sentiment = match_simple.group(0)\n",
        "         print(f\"Извлеченный сентимент (упрощенный поиск): {sentiment}\")\n",
        "    else:\n",
        "        print(\"Не удалось извлечь четкий сентимент (1, -1, 0) из ответа.\")\n",
        "print(\"-\" * 20)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T12:26:28.412549Z",
          "iopub.execute_input": "2025-05-01T12:26:28.413076Z",
          "iopub.status.idle": "2025-05-01T12:26:28.418515Z",
          "shell.execute_reply.started": "2025-05-01T12:26:28.413043Z",
          "shell.execute_reply": "2025-05-01T12:26:28.41784Z"
        },
        "id": "52tbifSabrMZ",
        "outputId": "ac18561b-2dc5-40c7-dfb2-6cbfc213628f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ошибка при применении chat template: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating\nЭто может случиться, если токенизатор не имеет стандартного шаблона чата.\nПопробуйте отформатировать промпт вручную (менее предпочтительно):\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_sber = \"Ты - финансовый аналитик с большим опытом в инвестировании. Твоя задача состоит в анализе сентимента публикаций из социальной сети Тинькофф Пульс. Верни только одно слово: 1 (новость положительная), -1 (новость негативная), 0 (нейтральный пост, если явно нельзя определить тональность)\"\n",
        "query1 = \"{$SBER} ракеетаа…)\"\n",
        "\n",
        "# 2. Создайте список сообщений для chat template\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": prompt_sber},\n",
        "    {\"role\": \"user\", \"content\": query1}\n",
        "]\n",
        "\n",
        "# 3. Примените chat template и токенизируйте\n",
        "#    Переместите результат на основное устройство модели (обычно cuda:0)\n",
        "#    ПРИМЕЧАНИЕ: apply_chat_template может работать лучше с моделями '-instruct'.\n",
        "#              С '-pretrain' результаты могут быть менее предсказуемыми.\n",
        "try:\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True, # Важно для моделей чата/инструкций при генерации\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device) # Используем model.device для корректного размещения тензора\n",
        "    print(f\"Input tensor placed on device: {input_ids.device}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при применении chat template: {e}\")\n",
        "    print(\"Это может случиться, если токенизатор не имеет стандартного шаблона чата.\")\n",
        "    print(\"Попробуйте отформатировать промпт вручную (менее предпочтительно):\")\n",
        "    # Как запасной вариант, если apply_chat_template не сработает:\n",
        "    # full_prompt = f\"System: {prompt_sber}\\nUser: {query1}\\nAssistant:\"\n",
        "    # input_ids = tokenizer(full_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    # Но лучше использовать apply_chat_template, если возможно.\n",
        "    # Если ошибка выше, дальнейший код может не сработать корректно.\n",
        "\n",
        "# 4. Запустите генерацию ответа моделью\n",
        "#    Уменьшено max_new_tokens, т.к. ожидается короткий ответ (1, -1 или 0)\n",
        "print(\"Генерация ответа...\")\n",
        "outputs = model.generate(input_ids, max_new_tokens=10) # Значительно уменьшено\n",
        "\n",
        "# 5. Декодируйте только сгенерированную часть ответа\n",
        "#    Срез outputs[0][input_ids.shape[-1]:] убирает токены входного промпта\n",
        "generated_token_ids = outputs[0][input_ids.shape[-1]:]\n",
        "response_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"-\" * 20)\n",
        "print(f\"Запрос: {query1}\")\n",
        "print(f\"Ответ модели (сырой): {response_text}\")\n",
        "\n",
        "# Опционально: Попытка извлечь только число из ответа\n",
        "import re\n",
        "match = re.search(r\"[-+]?\\b1\\b|\\b0\\b\", response_text.strip()) # Ищет 1, -1 или 0 как отдельные слова/числа\n",
        "if match:\n",
        "    sentiment = match.group(0)\n",
        "    print(f\"Извлеченный сентимент: {sentiment}\")\n",
        "else:\n",
        "    print(\"Не удалось извлечь четкий сентимент (1, -1, 0) из ответа.\")\n",
        "print(\"-\" * 20)"
      ],
      "metadata": {
        "trusted": true,
        "id": "FwEFSt1JbrMa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "d--cOZgnbrMa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Привет, как дела?\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs.to(model.device), max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "trusted": true,
        "id": "x7FWvH_ibrMa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "IinHV8aFbrMb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Ap5s8YLjbrMb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "TIp7EOn_brMb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Azbv-Et7brMb"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}